{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras\n",
    "Librería para programar redes neuronales de una manera más sencilla que con TensorFlow. Keras se encuentra en una capa de abstracción por encima de TensorFlow.\n",
    "\n",
    "[Documentación](https://keras.io/guides/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow\n",
    "# !pip install keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empezamos importando librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos los datos de mnist. No vamos a tratar imagenes con redes convolucionales (perdemos la estructura espacial 2D). Todos los pixeles se convertirán en un vector de 28x28 features independientes, que serán las entradas del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cogemos las imágenes de los dígitos asi como el conjunto de train y test\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos dimensiones del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "60.000 imagenes de 28x28 pixeles\n",
    "'''\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
       "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
       "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
       "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
       "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
       "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
       "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
       "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
       "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
       "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
       "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
       "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
       "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
       "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "60.000 imágenes de 28x28 pixeles. Vamos a representar una de ellas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(X_train[0], cmap=plt.cm.get_cmap('Greys'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada imagen se compone de 28x28 pixeles, y cada pixel representa una escala de grises que va del 0 al 255. Siendo 0 el blanco y 255 negro.\n",
    "\n",
    "¿Se te ocurre alguna manera de normalizar los datos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5019607843137255"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "128/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "255/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(\"float32\")/255\n",
    "X_test = X_test.astype(\"float32\")/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13066062"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.01176471, 0.07058824, 0.07058824,\n",
       "        0.07058824, 0.49411765, 0.53333336, 0.6862745 , 0.10196079,\n",
       "        0.6509804 , 1.        , 0.96862745, 0.49803922, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.11764706, 0.14117648,\n",
       "        0.36862746, 0.6039216 , 0.6666667 , 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.88235295, 0.6745098 ,\n",
       "        0.99215686, 0.9490196 , 0.7647059 , 0.2509804 , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.19215687, 0.93333334, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.9843137 , 0.3647059 , 0.32156864,\n",
       "        0.32156864, 0.21960784, 0.15294118, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.07058824, 0.85882354, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.7764706 ,\n",
       "        0.7137255 , 0.96862745, 0.94509804, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.3137255 , 0.6117647 ,\n",
       "        0.41960785, 0.99215686, 0.99215686, 0.8039216 , 0.04313726,\n",
       "        0.        , 0.16862746, 0.6039216 , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.05490196,\n",
       "        0.00392157, 0.6039216 , 0.99215686, 0.3529412 , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.54509807, 0.99215686, 0.74509805, 0.00784314,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.04313726, 0.74509805, 0.99215686, 0.27450982,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.13725491, 0.94509804, 0.88235295,\n",
       "        0.627451  , 0.42352942, 0.00392157, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.31764707, 0.9411765 ,\n",
       "        0.99215686, 0.99215686, 0.46666667, 0.09803922, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.1764706 ,\n",
       "        0.7294118 , 0.99215686, 0.99215686, 0.5882353 , 0.10588235,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.0627451 , 0.3647059 , 0.9882353 , 0.99215686, 0.73333335,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.9764706 , 0.99215686, 0.9764706 ,\n",
       "        0.2509804 , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.18039216,\n",
       "        0.50980395, 0.7176471 , 0.99215686, 0.99215686, 0.8117647 ,\n",
       "        0.00784314, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.15294118, 0.5803922 , 0.8980392 ,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.98039216, 0.7137255 ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.09411765, 0.44705883, 0.8666667 , 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.7882353 , 0.30588236, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.09019608, 0.25882354,\n",
       "        0.8352941 , 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "        0.7764706 , 0.31764707, 0.00784314, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.07058824, 0.67058825, 0.85882354, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.7647059 , 0.3137255 ,\n",
       "        0.03529412, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.21568628,\n",
       "        0.6745098 , 0.8862745 , 0.99215686, 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.95686275, 0.52156866, 0.04313726, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.53333336,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.83137256, 0.5294118 ,\n",
       "        0.5176471 , 0.0627451 , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Comprobamos la normalización\n",
    "'''\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.astype(\"float32\")\n",
    "y_test = y_test.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardamos datos para validación. Estos datos se usarán durante el entrenamiento. Otra opción es decirle a keras en la etapa de entrenamiento que reserve un X % de los datos para validar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = X_train[-10000:]\n",
    "y_val = y_train[-10000:]\n",
    "\n",
    "X_train = X_train[:-10000]\n",
    "y_train = y_train[:-10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 28, 28)\n",
      "(10000, 28, 28)\n",
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Montamos la arquitectura de la red neuronal. Se va a componer de:\n",
    "* **Sequential**: API para iniciar la red neuronal. No cuenta como capa.\n",
    "* **Flatten**: capa de entrada. Necesita un vector unidimensional. Como tenemos imágenes, esta capa aplana las imagenes (2D) en 1D.\n",
    "* **Dense**: es una hidden layer. Se compondrá de `n` neuronas y de una función de activación que se aplicará a todas las neuronas de la capa.\n",
    "\n",
    "Recuerda que es un problema de clasificación multiclase (10 clases) y que por tanto la última capa se compondrá de tantas neuronas como clases tengas.\n",
    "\n",
    "En cuanto a las funciones de activación es recomendable usar relu en las hidden layer, que tarda menos en entrenar, mientras que la ultima (output) suele ser una softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "28*28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "\n",
    "# Capa entrada\n",
    "model.add(keras.layers.Flatten(input_shape=(28, 28)))\n",
    "\n",
    "# Hidden layer\n",
    "model.add(keras.layers.Dense(units = 300,\n",
    "                            activation='relu'))\n",
    "\n",
    "# Hidden layer\n",
    "model.add(keras.layers.Dense(units = 100,\n",
    "                            activation='relu'))\n",
    "\n",
    "# Capa salida\n",
    "model.add(keras.layers.Dense(units = 10,\n",
    "                            activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Otra manera de declarar la red neuronal\n",
    "capas = [\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(units = 300, activation='relu'),\n",
    "    keras.layers.Dense(units = 100, activation='relu'),\n",
    "    keras.layers.Dense(units = 10, activation='softmax')\n",
    "]\n",
    "\n",
    "model = keras.models.Sequential(capas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver las capas, y acceder a sus elementos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.layers.core.flatten.Flatten object at 0x000001B94806D648>\n"
     ]
    }
   ],
   "source": [
    "print(model.layers[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver los pesos de las capas sin entrenar, porque los inicializa aleatoriamente. Los bias los inicializa a 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden1 = model.layers[1]\n",
    "weights, biases = hidden1.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04026736,  0.05425833,  0.03832676, ...,  0.01311453,\n",
       "         0.05822879,  0.06099585],\n",
       "       [ 0.06912588,  0.05660552, -0.02627824, ..., -0.06167023,\n",
       "         0.04953351, -0.05638317],\n",
       "       [-0.05003565,  0.00192373, -0.0095281 , ..., -0.00089435,\n",
       "        -0.01069951, -0.06065356],\n",
       "       ...,\n",
       "       [ 0.03444975, -0.01066724,  0.06942323, ...,  0.02369119,\n",
       "         0.01562557,  0.01007675],\n",
       "       [ 0.03696927,  0.02191073, -0.03178998, ...,  0.03268629,\n",
       "         0.01911028,  0.04473957],\n",
       "       [-0.06383806, -0.01695288, -0.0479633 , ...,  0.05210909,\n",
       "         0.00283075, -0.06215543]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(weights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "28*28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235200"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "300*784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235200"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establecemos la configuración de ejecución... el compile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = keras.optimizers.SGD(),\n",
    "    loss = keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics = [keras.metrics.SparseCategoricalAccuracy()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equivalente\n",
    "model.compile(\n",
    "    optimizer = \"sgd\",\n",
    "    loss = \"sparse_categorical_crossentropy\",\n",
    "    metrics = [\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_1 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 300)               235500    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 100)               30100     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235500\n"
     ]
    }
   ],
   "source": [
    "# 1º neurona de la 1º hidden layer\n",
    "# y = a + w1*x1 + w2*x2 + .... wn*xn\n",
    "# a es el intercepto llamado bias\n",
    "# wn es cada uno de los pesos que va a ir actualizando con el backpropagation\n",
    "# n es 784\n",
    "# En la 1º hidden layer tenemos 784 pesos por cada neurona, al tener 300, tenemos un total de:\n",
    "print(784*300 + 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235500"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "300 * 784 + 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30100"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "300 * 100 + 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1010"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100 * 10 + 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos el modelo. Usamos los datos de entrenamiento. El batch_size es la cantidad de muestras que utiliza el SGD, y las epochs son las iteraciones que realiza en el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 28, 28)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "390.625"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "50000/128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 1.2334 - accuracy: 0.7032 - val_loss: 0.5957 - val_accuracy: 0.8669\n",
      "Epoch 2/50\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 0.5169 - accuracy: 0.8669 - val_loss: 0.3946 - val_accuracy: 0.8973\n",
      "Epoch 3/50\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 0.3988 - accuracy: 0.8912 - val_loss: 0.3336 - val_accuracy: 0.9109\n",
      "Epoch 4/50\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 0.3497 - accuracy: 0.9018 - val_loss: 0.3036 - val_accuracy: 0.9170\n",
      "Epoch 5/50\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.3203 - accuracy: 0.9097 - val_loss: 0.2846 - val_accuracy: 0.9211\n",
      "Epoch 6/50\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.2994 - accuracy: 0.9155 - val_loss: 0.2689 - val_accuracy: 0.9250\n",
      "Epoch 7/50\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.2829 - accuracy: 0.9197 - val_loss: 0.2568 - val_accuracy: 0.9271\n",
      "Epoch 8/50\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.2689 - accuracy: 0.9241 - val_loss: 0.2448 - val_accuracy: 0.9303\n",
      "Epoch 9/50\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.2569 - accuracy: 0.9277 - val_loss: 0.2360 - val_accuracy: 0.9320\n",
      "Epoch 10/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.2461 - accuracy: 0.9305 - val_loss: 0.2272 - val_accuracy: 0.9342\n",
      "Epoch 11/50\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.2364 - accuracy: 0.9335 - val_loss: 0.2196 - val_accuracy: 0.9371\n",
      "Epoch 12/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.2271 - accuracy: 0.9361 - val_loss: 0.2144 - val_accuracy: 0.9382\n",
      "Epoch 13/50\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.2190 - accuracy: 0.9383 - val_loss: 0.2038 - val_accuracy: 0.9439\n",
      "Epoch 14/50\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.2111 - accuracy: 0.9406 - val_loss: 0.2000 - val_accuracy: 0.9448\n",
      "Epoch 15/50\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.2036 - accuracy: 0.9426 - val_loss: 0.1923 - val_accuracy: 0.9466\n",
      "Epoch 16/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1970 - accuracy: 0.9444 - val_loss: 0.1872 - val_accuracy: 0.9481\n",
      "Epoch 17/50\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 0.1905 - accuracy: 0.9460 - val_loss: 0.1817 - val_accuracy: 0.9494\n",
      "Epoch 18/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1844 - accuracy: 0.9469 - val_loss: 0.1764 - val_accuracy: 0.9516\n",
      "Epoch 19/50\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1783 - accuracy: 0.9488 - val_loss: 0.1755 - val_accuracy: 0.9529\n",
      "Epoch 20/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1731 - accuracy: 0.9499 - val_loss: 0.1679 - val_accuracy: 0.9543\n",
      "Epoch 21/50\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.1679 - accuracy: 0.9514 - val_loss: 0.1641 - val_accuracy: 0.9553\n",
      "Epoch 22/50\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.1631 - accuracy: 0.9534 - val_loss: 0.1611 - val_accuracy: 0.9551\n",
      "Epoch 23/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1584 - accuracy: 0.9542 - val_loss: 0.1567 - val_accuracy: 0.9576\n",
      "Epoch 24/50\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.1541 - accuracy: 0.9557 - val_loss: 0.1530 - val_accuracy: 0.9578\n",
      "Epoch 25/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1496 - accuracy: 0.9570 - val_loss: 0.1503 - val_accuracy: 0.9578\n",
      "Epoch 26/50\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.1457 - accuracy: 0.9580 - val_loss: 0.1490 - val_accuracy: 0.9581\n",
      "Epoch 27/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1418 - accuracy: 0.9591 - val_loss: 0.1454 - val_accuracy: 0.9603\n",
      "Epoch 28/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1382 - accuracy: 0.9606 - val_loss: 0.1418 - val_accuracy: 0.9607\n",
      "Epoch 29/50\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.1346 - accuracy: 0.9616 - val_loss: 0.1402 - val_accuracy: 0.9611\n",
      "Epoch 30/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1313 - accuracy: 0.9624 - val_loss: 0.1376 - val_accuracy: 0.9627\n",
      "Epoch 31/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1280 - accuracy: 0.9636 - val_loss: 0.1342 - val_accuracy: 0.9625\n",
      "Epoch 32/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1247 - accuracy: 0.9643 - val_loss: 0.1323 - val_accuracy: 0.9630\n",
      "Epoch 33/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1217 - accuracy: 0.9657 - val_loss: 0.1295 - val_accuracy: 0.9640\n",
      "Epoch 34/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1189 - accuracy: 0.9664 - val_loss: 0.1284 - val_accuracy: 0.9639\n",
      "Epoch 35/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1162 - accuracy: 0.9672 - val_loss: 0.1261 - val_accuracy: 0.9645\n",
      "Epoch 36/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1136 - accuracy: 0.9679 - val_loss: 0.1237 - val_accuracy: 0.9657\n",
      "Epoch 37/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1110 - accuracy: 0.9694 - val_loss: 0.1221 - val_accuracy: 0.9657\n",
      "Epoch 38/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1083 - accuracy: 0.9700 - val_loss: 0.1218 - val_accuracy: 0.9663\n",
      "Epoch 39/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1060 - accuracy: 0.9707 - val_loss: 0.1182 - val_accuracy: 0.9675\n",
      "Epoch 40/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1037 - accuracy: 0.9713 - val_loss: 0.1167 - val_accuracy: 0.9675\n",
      "Epoch 41/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1014 - accuracy: 0.9715 - val_loss: 0.1161 - val_accuracy: 0.9678: 0s - loss: 0.100\n",
      "Epoch 42/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.0993 - accuracy: 0.9731 - val_loss: 0.1141 - val_accuracy: 0.9682\n",
      "Epoch 43/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.0973 - accuracy: 0.9736 - val_loss: 0.1138 - val_accuracy: 0.9679\n",
      "Epoch 44/50\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.0950 - accuracy: 0.9739 - val_loss: 0.1125 - val_accuracy: 0.9690\n",
      "Epoch 45/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.0932 - accuracy: 0.9744 - val_loss: 0.1105 - val_accuracy: 0.9687\n",
      "Epoch 46/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.0914 - accuracy: 0.9749 - val_loss: 0.1094 - val_accuracy: 0.9700\n",
      "Epoch 47/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.0894 - accuracy: 0.9758 - val_loss: 0.1083 - val_accuracy: 0.9704\n",
      "Epoch 48/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.0877 - accuracy: 0.9765 - val_loss: 0.1077 - val_accuracy: 0.9701\n",
      "Epoch 49/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.0859 - accuracy: 0.9765 - val_loss: 0.1063 - val_accuracy: 0.9709\n",
      "Epoch 50/50\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.0842 - accuracy: 0.9775 - val_loss: 0.1055 - val_accuracy: 0.9708\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size = 128,\n",
    "    epochs = 50,\n",
    "    validation_data = (X_val, y_val) # validation_split = 0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos reentrenar el modelo. No empieza de nuevo, sino que retoma el entrenamiento anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0837 - accuracy: 0.9771 - val_loss: 0.1027 - val_accuracy: 0.9709\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0804 - accuracy: 0.9781 - val_loss: 0.1026 - val_accuracy: 0.9698\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0777 - accuracy: 0.9788 - val_loss: 0.1006 - val_accuracy: 0.9720\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0747 - accuracy: 0.9797 - val_loss: 0.0989 - val_accuracy: 0.9721\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0720 - accuracy: 0.9803 - val_loss: 0.0983 - val_accuracy: 0.9730\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0694 - accuracy: 0.9813 - val_loss: 0.0962 - val_accuracy: 0.9724\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0670 - accuracy: 0.9819 - val_loss: 0.0985 - val_accuracy: 0.9723\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0648 - accuracy: 0.9827 - val_loss: 0.0924 - val_accuracy: 0.9734\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0624 - accuracy: 0.9835 - val_loss: 0.0911 - val_accuracy: 0.9731\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0603 - accuracy: 0.9841 - val_loss: 0.0907 - val_accuracy: 0.9736\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b948247308>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size = 64,\n",
    "    epochs = 10,\n",
    "    validation_data = (X_val, y_val) # validation_split = 0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos el histórico del entrenamiento, para poder representarlo posteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': [1.2333921194076538, 0.5169460773468018, 0.3988115191459656, 0.3496546745300293, 0.32025814056396484, 0.2993830144405365, 0.2828635275363922, 0.2688817083835602, 0.2569364607334137, 0.24610312283039093, 0.23643581569194794, 0.22705557942390442, 0.21897351741790771, 0.21113602817058563, 0.2036178857088089, 0.19698020815849304, 0.19047130644321442, 0.1843889206647873, 0.1783166229724884, 0.17313021421432495, 0.1678769588470459, 0.1630650907754898, 0.1583949476480484, 0.15405811369419098, 0.14960086345672607, 0.14569514989852905, 0.14184866845607758, 0.13818015158176422, 0.13464532792568207, 0.13128133118152618, 0.12796452641487122, 0.12470230460166931, 0.12166677415370941, 0.11885364353656769, 0.1162007600069046, 0.11356765776872635, 0.11097307503223419, 0.10830425471067429, 0.10603540390729904, 0.10365214198827744, 0.10139203071594238, 0.09926805645227432, 0.09726358205080032, 0.09504131227731705, 0.09317939728498459, 0.09139199554920197, 0.08942419290542603, 0.08767597377300262, 0.08589702099561691, 0.08417532593011856], 'accuracy': [0.7031800150871277, 0.8669400215148926, 0.8911600112915039, 0.9018200039863586, 0.9096999764442444, 0.9154599905014038, 0.9197400212287903, 0.9240999817848206, 0.9276599884033203, 0.9304599761962891, 0.9334800243377686, 0.9361400008201599, 0.9382799863815308, 0.9406200051307678, 0.9426199793815613, 0.9444400072097778, 0.9460200071334839, 0.9468799829483032, 0.9488400220870972, 0.9499199986457825, 0.9513999819755554, 0.9534199833869934, 0.954200029373169, 0.9556800127029419, 0.9570000171661377, 0.9580000042915344, 0.9590799808502197, 0.9606199860572815, 0.9615799784660339, 0.962440013885498, 0.9635999798774719, 0.9642599821090698, 0.9656999707221985, 0.9663800001144409, 0.9672200083732605, 0.9678599834442139, 0.9693800210952759, 0.9700199961662292, 0.9706799983978271, 0.9712799787521362, 0.9714999794960022, 0.9730600118637085, 0.9736400246620178, 0.9739199876785278, 0.974399983882904, 0.9748600125312805, 0.9757999777793884, 0.9764599800109863, 0.9765200018882751, 0.9774799942970276], 'val_loss': [0.5957443118095398, 0.3946439027786255, 0.3335813581943512, 0.3035871088504791, 0.2846165895462036, 0.26889097690582275, 0.2567538917064667, 0.24483193457126617, 0.2360357940196991, 0.22716699540615082, 0.21964094042778015, 0.2144164741039276, 0.20384053885936737, 0.199953094124794, 0.19230236113071442, 0.18716159462928772, 0.1817195564508438, 0.17642195522785187, 0.17552073299884796, 0.16791407763957977, 0.16407011449337006, 0.1610516756772995, 0.15674525499343872, 0.1530463695526123, 0.15026932954788208, 0.14904429018497467, 0.1454077512025833, 0.1417795866727829, 0.14018529653549194, 0.13756577670574188, 0.13420015573501587, 0.1323288530111313, 0.129496768116951, 0.12838831543922424, 0.1261470913887024, 0.12372040003538132, 0.12214209139347076, 0.1217513158917427, 0.11818618327379227, 0.11666838079690933, 0.11613418161869049, 0.11411451548337936, 0.11377538740634918, 0.11251039803028107, 0.11050328612327576, 0.10943188518285751, 0.1082707867026329, 0.1076594814658165, 0.10626127570867538, 0.10550183057785034], 'val_accuracy': [0.8669000267982483, 0.8973000049591064, 0.9108999967575073, 0.9169999957084656, 0.9211000204086304, 0.925000011920929, 0.9271000027656555, 0.9302999973297119, 0.9319999814033508, 0.9341999888420105, 0.9370999932289124, 0.9381999969482422, 0.9438999891281128, 0.9448000192642212, 0.9466000199317932, 0.9480999708175659, 0.949400007724762, 0.9516000151634216, 0.9528999924659729, 0.9542999863624573, 0.955299973487854, 0.9550999999046326, 0.9575999975204468, 0.9577999711036682, 0.9577999711036682, 0.9581000208854675, 0.9603000283241272, 0.9606999754905701, 0.9610999822616577, 0.9627000093460083, 0.9624999761581421, 0.9629999995231628, 0.9639999866485596, 0.9639000296592712, 0.9645000100135803, 0.9656999707221985, 0.9656999707221985, 0.9663000106811523, 0.9674999713897705, 0.9674999713897705, 0.9678000211715698, 0.9682000279426575, 0.9678999781608582, 0.968999981880188, 0.9686999917030334, 0.9700000286102295, 0.9703999757766724, 0.9700999855995178, 0.9708999991416931, 0.97079998254776]}\n"
     ]
    }
   ],
   "source": [
    "# print(history.params)\n",
    "# print(history.epoch)\n",
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [1.2333921194076538,\n",
       "  0.5169460773468018,\n",
       "  0.3988115191459656,\n",
       "  0.3496546745300293,\n",
       "  0.32025814056396484,\n",
       "  0.2993830144405365,\n",
       "  0.2828635275363922,\n",
       "  0.2688817083835602,\n",
       "  0.2569364607334137,\n",
       "  0.24610312283039093,\n",
       "  0.23643581569194794,\n",
       "  0.22705557942390442,\n",
       "  0.21897351741790771,\n",
       "  0.21113602817058563,\n",
       "  0.2036178857088089,\n",
       "  0.19698020815849304,\n",
       "  0.19047130644321442,\n",
       "  0.1843889206647873,\n",
       "  0.1783166229724884,\n",
       "  0.17313021421432495,\n",
       "  0.1678769588470459,\n",
       "  0.1630650907754898,\n",
       "  0.1583949476480484,\n",
       "  0.15405811369419098,\n",
       "  0.14960086345672607,\n",
       "  0.14569514989852905,\n",
       "  0.14184866845607758,\n",
       "  0.13818015158176422,\n",
       "  0.13464532792568207,\n",
       "  0.13128133118152618,\n",
       "  0.12796452641487122,\n",
       "  0.12470230460166931,\n",
       "  0.12166677415370941,\n",
       "  0.11885364353656769,\n",
       "  0.1162007600069046,\n",
       "  0.11356765776872635,\n",
       "  0.11097307503223419,\n",
       "  0.10830425471067429,\n",
       "  0.10603540390729904,\n",
       "  0.10365214198827744,\n",
       "  0.10139203071594238,\n",
       "  0.09926805645227432,\n",
       "  0.09726358205080032,\n",
       "  0.09504131227731705,\n",
       "  0.09317939728498459,\n",
       "  0.09139199554920197,\n",
       "  0.08942419290542603,\n",
       "  0.08767597377300262,\n",
       "  0.08589702099561691,\n",
       "  0.08417532593011856],\n",
       " 'accuracy': [0.7031800150871277,\n",
       "  0.8669400215148926,\n",
       "  0.8911600112915039,\n",
       "  0.9018200039863586,\n",
       "  0.9096999764442444,\n",
       "  0.9154599905014038,\n",
       "  0.9197400212287903,\n",
       "  0.9240999817848206,\n",
       "  0.9276599884033203,\n",
       "  0.9304599761962891,\n",
       "  0.9334800243377686,\n",
       "  0.9361400008201599,\n",
       "  0.9382799863815308,\n",
       "  0.9406200051307678,\n",
       "  0.9426199793815613,\n",
       "  0.9444400072097778,\n",
       "  0.9460200071334839,\n",
       "  0.9468799829483032,\n",
       "  0.9488400220870972,\n",
       "  0.9499199986457825,\n",
       "  0.9513999819755554,\n",
       "  0.9534199833869934,\n",
       "  0.954200029373169,\n",
       "  0.9556800127029419,\n",
       "  0.9570000171661377,\n",
       "  0.9580000042915344,\n",
       "  0.9590799808502197,\n",
       "  0.9606199860572815,\n",
       "  0.9615799784660339,\n",
       "  0.962440013885498,\n",
       "  0.9635999798774719,\n",
       "  0.9642599821090698,\n",
       "  0.9656999707221985,\n",
       "  0.9663800001144409,\n",
       "  0.9672200083732605,\n",
       "  0.9678599834442139,\n",
       "  0.9693800210952759,\n",
       "  0.9700199961662292,\n",
       "  0.9706799983978271,\n",
       "  0.9712799787521362,\n",
       "  0.9714999794960022,\n",
       "  0.9730600118637085,\n",
       "  0.9736400246620178,\n",
       "  0.9739199876785278,\n",
       "  0.974399983882904,\n",
       "  0.9748600125312805,\n",
       "  0.9757999777793884,\n",
       "  0.9764599800109863,\n",
       "  0.9765200018882751,\n",
       "  0.9774799942970276],\n",
       " 'val_loss': [0.5957443118095398,\n",
       "  0.3946439027786255,\n",
       "  0.3335813581943512,\n",
       "  0.3035871088504791,\n",
       "  0.2846165895462036,\n",
       "  0.26889097690582275,\n",
       "  0.2567538917064667,\n",
       "  0.24483193457126617,\n",
       "  0.2360357940196991,\n",
       "  0.22716699540615082,\n",
       "  0.21964094042778015,\n",
       "  0.2144164741039276,\n",
       "  0.20384053885936737,\n",
       "  0.199953094124794,\n",
       "  0.19230236113071442,\n",
       "  0.18716159462928772,\n",
       "  0.1817195564508438,\n",
       "  0.17642195522785187,\n",
       "  0.17552073299884796,\n",
       "  0.16791407763957977,\n",
       "  0.16407011449337006,\n",
       "  0.1610516756772995,\n",
       "  0.15674525499343872,\n",
       "  0.1530463695526123,\n",
       "  0.15026932954788208,\n",
       "  0.14904429018497467,\n",
       "  0.1454077512025833,\n",
       "  0.1417795866727829,\n",
       "  0.14018529653549194,\n",
       "  0.13756577670574188,\n",
       "  0.13420015573501587,\n",
       "  0.1323288530111313,\n",
       "  0.129496768116951,\n",
       "  0.12838831543922424,\n",
       "  0.1261470913887024,\n",
       "  0.12372040003538132,\n",
       "  0.12214209139347076,\n",
       "  0.1217513158917427,\n",
       "  0.11818618327379227,\n",
       "  0.11666838079690933,\n",
       "  0.11613418161869049,\n",
       "  0.11411451548337936,\n",
       "  0.11377538740634918,\n",
       "  0.11251039803028107,\n",
       "  0.11050328612327576,\n",
       "  0.10943188518285751,\n",
       "  0.1082707867026329,\n",
       "  0.1076594814658165,\n",
       "  0.10626127570867538,\n",
       "  0.10550183057785034],\n",
       " 'val_accuracy': [0.8669000267982483,\n",
       "  0.8973000049591064,\n",
       "  0.9108999967575073,\n",
       "  0.9169999957084656,\n",
       "  0.9211000204086304,\n",
       "  0.925000011920929,\n",
       "  0.9271000027656555,\n",
       "  0.9302999973297119,\n",
       "  0.9319999814033508,\n",
       "  0.9341999888420105,\n",
       "  0.9370999932289124,\n",
       "  0.9381999969482422,\n",
       "  0.9438999891281128,\n",
       "  0.9448000192642212,\n",
       "  0.9466000199317932,\n",
       "  0.9480999708175659,\n",
       "  0.949400007724762,\n",
       "  0.9516000151634216,\n",
       "  0.9528999924659729,\n",
       "  0.9542999863624573,\n",
       "  0.955299973487854,\n",
       "  0.9550999999046326,\n",
       "  0.9575999975204468,\n",
       "  0.9577999711036682,\n",
       "  0.9577999711036682,\n",
       "  0.9581000208854675,\n",
       "  0.9603000283241272,\n",
       "  0.9606999754905701,\n",
       "  0.9610999822616577,\n",
       "  0.9627000093460083,\n",
       "  0.9624999761581421,\n",
       "  0.9629999995231628,\n",
       "  0.9639999866485596,\n",
       "  0.9639000296592712,\n",
       "  0.9645000100135803,\n",
       "  0.9656999707221985,\n",
       "  0.9656999707221985,\n",
       "  0.9663000106811523,\n",
       "  0.9674999713897705,\n",
       "  0.9674999713897705,\n",
       "  0.9678000211715698,\n",
       "  0.9682000279426575,\n",
       "  0.9678999781608582,\n",
       "  0.968999981880188,\n",
       "  0.9686999917030334,\n",
       "  0.9700000286102295,\n",
       "  0.9703999757766724,\n",
       "  0.9700999855995178,\n",
       "  0.9708999991416931,\n",
       "  0.97079998254776]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.233392</td>\n",
       "      <td>0.70318</td>\n",
       "      <td>0.595744</td>\n",
       "      <td>0.8669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.516946</td>\n",
       "      <td>0.86694</td>\n",
       "      <td>0.394644</td>\n",
       "      <td>0.8973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.398812</td>\n",
       "      <td>0.89116</td>\n",
       "      <td>0.333581</td>\n",
       "      <td>0.9109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.349655</td>\n",
       "      <td>0.90182</td>\n",
       "      <td>0.303587</td>\n",
       "      <td>0.9170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.320258</td>\n",
       "      <td>0.90970</td>\n",
       "      <td>0.284617</td>\n",
       "      <td>0.9211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.299383</td>\n",
       "      <td>0.91546</td>\n",
       "      <td>0.268891</td>\n",
       "      <td>0.9250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.282864</td>\n",
       "      <td>0.91974</td>\n",
       "      <td>0.256754</td>\n",
       "      <td>0.9271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.268882</td>\n",
       "      <td>0.92410</td>\n",
       "      <td>0.244832</td>\n",
       "      <td>0.9303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.256936</td>\n",
       "      <td>0.92766</td>\n",
       "      <td>0.236036</td>\n",
       "      <td>0.9320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.246103</td>\n",
       "      <td>0.93046</td>\n",
       "      <td>0.227167</td>\n",
       "      <td>0.9342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.236436</td>\n",
       "      <td>0.93348</td>\n",
       "      <td>0.219641</td>\n",
       "      <td>0.9371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.227056</td>\n",
       "      <td>0.93614</td>\n",
       "      <td>0.214416</td>\n",
       "      <td>0.9382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.218974</td>\n",
       "      <td>0.93828</td>\n",
       "      <td>0.203841</td>\n",
       "      <td>0.9439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.211136</td>\n",
       "      <td>0.94062</td>\n",
       "      <td>0.199953</td>\n",
       "      <td>0.9448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.203618</td>\n",
       "      <td>0.94262</td>\n",
       "      <td>0.192302</td>\n",
       "      <td>0.9466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.196980</td>\n",
       "      <td>0.94444</td>\n",
       "      <td>0.187162</td>\n",
       "      <td>0.9481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.190471</td>\n",
       "      <td>0.94602</td>\n",
       "      <td>0.181720</td>\n",
       "      <td>0.9494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.184389</td>\n",
       "      <td>0.94688</td>\n",
       "      <td>0.176422</td>\n",
       "      <td>0.9516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.178317</td>\n",
       "      <td>0.94884</td>\n",
       "      <td>0.175521</td>\n",
       "      <td>0.9529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.173130</td>\n",
       "      <td>0.94992</td>\n",
       "      <td>0.167914</td>\n",
       "      <td>0.9543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.167877</td>\n",
       "      <td>0.95140</td>\n",
       "      <td>0.164070</td>\n",
       "      <td>0.9553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.163065</td>\n",
       "      <td>0.95342</td>\n",
       "      <td>0.161052</td>\n",
       "      <td>0.9551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.158395</td>\n",
       "      <td>0.95420</td>\n",
       "      <td>0.156745</td>\n",
       "      <td>0.9576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.154058</td>\n",
       "      <td>0.95568</td>\n",
       "      <td>0.153046</td>\n",
       "      <td>0.9578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.149601</td>\n",
       "      <td>0.95700</td>\n",
       "      <td>0.150269</td>\n",
       "      <td>0.9578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.145695</td>\n",
       "      <td>0.95800</td>\n",
       "      <td>0.149044</td>\n",
       "      <td>0.9581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.141849</td>\n",
       "      <td>0.95908</td>\n",
       "      <td>0.145408</td>\n",
       "      <td>0.9603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.138180</td>\n",
       "      <td>0.96062</td>\n",
       "      <td>0.141780</td>\n",
       "      <td>0.9607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.134645</td>\n",
       "      <td>0.96158</td>\n",
       "      <td>0.140185</td>\n",
       "      <td>0.9611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.131281</td>\n",
       "      <td>0.96244</td>\n",
       "      <td>0.137566</td>\n",
       "      <td>0.9627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.127965</td>\n",
       "      <td>0.96360</td>\n",
       "      <td>0.134200</td>\n",
       "      <td>0.9625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.124702</td>\n",
       "      <td>0.96426</td>\n",
       "      <td>0.132329</td>\n",
       "      <td>0.9630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.121667</td>\n",
       "      <td>0.96570</td>\n",
       "      <td>0.129497</td>\n",
       "      <td>0.9640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.118854</td>\n",
       "      <td>0.96638</td>\n",
       "      <td>0.128388</td>\n",
       "      <td>0.9639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.116201</td>\n",
       "      <td>0.96722</td>\n",
       "      <td>0.126147</td>\n",
       "      <td>0.9645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.113568</td>\n",
       "      <td>0.96786</td>\n",
       "      <td>0.123720</td>\n",
       "      <td>0.9657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.110973</td>\n",
       "      <td>0.96938</td>\n",
       "      <td>0.122142</td>\n",
       "      <td>0.9657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.108304</td>\n",
       "      <td>0.97002</td>\n",
       "      <td>0.121751</td>\n",
       "      <td>0.9663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.106035</td>\n",
       "      <td>0.97068</td>\n",
       "      <td>0.118186</td>\n",
       "      <td>0.9675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.103652</td>\n",
       "      <td>0.97128</td>\n",
       "      <td>0.116668</td>\n",
       "      <td>0.9675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.101392</td>\n",
       "      <td>0.97150</td>\n",
       "      <td>0.116134</td>\n",
       "      <td>0.9678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.099268</td>\n",
       "      <td>0.97306</td>\n",
       "      <td>0.114115</td>\n",
       "      <td>0.9682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.097264</td>\n",
       "      <td>0.97364</td>\n",
       "      <td>0.113775</td>\n",
       "      <td>0.9679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.095041</td>\n",
       "      <td>0.97392</td>\n",
       "      <td>0.112510</td>\n",
       "      <td>0.9690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.093179</td>\n",
       "      <td>0.97440</td>\n",
       "      <td>0.110503</td>\n",
       "      <td>0.9687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.091392</td>\n",
       "      <td>0.97486</td>\n",
       "      <td>0.109432</td>\n",
       "      <td>0.9700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.089424</td>\n",
       "      <td>0.97580</td>\n",
       "      <td>0.108271</td>\n",
       "      <td>0.9704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.087676</td>\n",
       "      <td>0.97646</td>\n",
       "      <td>0.107659</td>\n",
       "      <td>0.9701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.085897</td>\n",
       "      <td>0.97652</td>\n",
       "      <td>0.106261</td>\n",
       "      <td>0.9709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.084175</td>\n",
       "      <td>0.97748</td>\n",
       "      <td>0.105502</td>\n",
       "      <td>0.9708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        loss  accuracy  val_loss  val_accuracy\n",
       "0   1.233392   0.70318  0.595744        0.8669\n",
       "1   0.516946   0.86694  0.394644        0.8973\n",
       "2   0.398812   0.89116  0.333581        0.9109\n",
       "3   0.349655   0.90182  0.303587        0.9170\n",
       "4   0.320258   0.90970  0.284617        0.9211\n",
       "5   0.299383   0.91546  0.268891        0.9250\n",
       "6   0.282864   0.91974  0.256754        0.9271\n",
       "7   0.268882   0.92410  0.244832        0.9303\n",
       "8   0.256936   0.92766  0.236036        0.9320\n",
       "9   0.246103   0.93046  0.227167        0.9342\n",
       "10  0.236436   0.93348  0.219641        0.9371\n",
       "11  0.227056   0.93614  0.214416        0.9382\n",
       "12  0.218974   0.93828  0.203841        0.9439\n",
       "13  0.211136   0.94062  0.199953        0.9448\n",
       "14  0.203618   0.94262  0.192302        0.9466\n",
       "15  0.196980   0.94444  0.187162        0.9481\n",
       "16  0.190471   0.94602  0.181720        0.9494\n",
       "17  0.184389   0.94688  0.176422        0.9516\n",
       "18  0.178317   0.94884  0.175521        0.9529\n",
       "19  0.173130   0.94992  0.167914        0.9543\n",
       "20  0.167877   0.95140  0.164070        0.9553\n",
       "21  0.163065   0.95342  0.161052        0.9551\n",
       "22  0.158395   0.95420  0.156745        0.9576\n",
       "23  0.154058   0.95568  0.153046        0.9578\n",
       "24  0.149601   0.95700  0.150269        0.9578\n",
       "25  0.145695   0.95800  0.149044        0.9581\n",
       "26  0.141849   0.95908  0.145408        0.9603\n",
       "27  0.138180   0.96062  0.141780        0.9607\n",
       "28  0.134645   0.96158  0.140185        0.9611\n",
       "29  0.131281   0.96244  0.137566        0.9627\n",
       "30  0.127965   0.96360  0.134200        0.9625\n",
       "31  0.124702   0.96426  0.132329        0.9630\n",
       "32  0.121667   0.96570  0.129497        0.9640\n",
       "33  0.118854   0.96638  0.128388        0.9639\n",
       "34  0.116201   0.96722  0.126147        0.9645\n",
       "35  0.113568   0.96786  0.123720        0.9657\n",
       "36  0.110973   0.96938  0.122142        0.9657\n",
       "37  0.108304   0.97002  0.121751        0.9663\n",
       "38  0.106035   0.97068  0.118186        0.9675\n",
       "39  0.103652   0.97128  0.116668        0.9675\n",
       "40  0.101392   0.97150  0.116134        0.9678\n",
       "41  0.099268   0.97306  0.114115        0.9682\n",
       "42  0.097264   0.97364  0.113775        0.9679\n",
       "43  0.095041   0.97392  0.112510        0.9690\n",
       "44  0.093179   0.97440  0.110503        0.9687\n",
       "45  0.091392   0.97486  0.109432        0.9700\n",
       "46  0.089424   0.97580  0.108271        0.9704\n",
       "47  0.087676   0.97646  0.107659        0.9701\n",
       "48  0.085897   0.97652  0.106261        0.9709\n",
       "49  0.084175   0.97748  0.105502        0.9708"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si el modelo no ha ido bien, prueba a cambiar el learning rate, cambia de optimizador y después prueba a cambiar capas, neuronas y funciones de activación.\n",
    "\n",
    "Ya tenemos el modelo entrenado. Probémoslo con test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0876 - accuracy: 0.9736\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.08762284368276596, 0.9735999703407288]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cogemos el primero\n",
    "plt.imshow(X_test[0].reshape(28,28), cmap=plt.cm.get_cmap('Greys'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.32941177, 0.7254902 , 0.62352943, 0.5921569 ,\n",
       "         0.23529412, 0.14117648, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.87058824, 0.99607843, 0.99607843, 0.99607843,\n",
       "         0.99607843, 0.94509804, 0.7764706 , 0.7764706 , 0.7764706 ,\n",
       "         0.7764706 , 0.7764706 , 0.7764706 , 0.7764706 , 0.7764706 ,\n",
       "         0.6666667 , 0.20392157, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.2627451 , 0.44705883, 0.28235295, 0.44705883,\n",
       "         0.6392157 , 0.8901961 , 0.99607843, 0.88235295, 0.99607843,\n",
       "         0.99607843, 0.99607843, 0.98039216, 0.8980392 , 0.99607843,\n",
       "         0.99607843, 0.54901963, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.06666667, 0.25882354, 0.05490196, 0.2627451 ,\n",
       "         0.2627451 , 0.2627451 , 0.23137255, 0.08235294, 0.9254902 ,\n",
       "         0.99607843, 0.41568628, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.3254902 , 0.99215686,\n",
       "         0.81960785, 0.07058824, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.08627451, 0.9137255 , 1.        ,\n",
       "         0.3254902 , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.5058824 , 0.99607843, 0.93333334,\n",
       "         0.17254902, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.23137255, 0.9764706 , 0.99607843, 0.24313726,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.52156866, 0.99607843, 0.73333335, 0.01960784,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.03529412, 0.8039216 , 0.972549  , 0.22745098, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.49411765, 0.99607843, 0.7137255 , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.29411766,\n",
       "         0.9843137 , 0.9411765 , 0.22352941, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.07450981, 0.8666667 ,\n",
       "         0.99607843, 0.6509804 , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.01176471, 0.79607844, 0.99607843,\n",
       "         0.85882354, 0.13725491, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.14901961, 0.99607843, 0.99607843,\n",
       "         0.3019608 , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.12156863, 0.8784314 , 0.99607843, 0.4509804 ,\n",
       "         0.00392157, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.52156866, 0.99607843, 0.99607843, 0.20392157,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.23921569, 0.9490196 , 0.99607843, 0.99607843, 0.20392157,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.4745098 , 0.99607843, 0.99607843, 0.85882354, 0.15686275,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.4745098 , 0.99607843, 0.8117647 , 0.07058824, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ]]], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[5.8637370e-06, 1.3507524e-06, 1.3008519e-04, 1.0975284e-03,\n",
       "        8.2203975e-08, 1.1930124e-05, 2.9907431e-11, 9.9871135e-01,\n",
       "        7.2743237e-06, 3.4365814e-05]], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(X_test[:1])\n",
    "print(predictions.shape)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, ..., 4, 5, 6], dtype=int64)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_test[2].reshape(28,28), cmap=plt.cm.get_cmap('Greys'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problema de regresión\n",
    "Veamos un ejemplo de cómo aplicar una red neuronal de TensorFlow a un problema de regresión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "      <td>4.526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "      <td>3.585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "      <td>3.521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "\n",
       "   Longitude  target  \n",
       "0    -122.23   4.526  \n",
       "1    -122.22   3.585  \n",
       "2    -122.24   3.521  \n",
       "3    -122.25   3.413  \n",
       "4    -122.25   3.422  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargamos datos\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "df = pd.DataFrame(housing.data, columns = housing.feature_names)\n",
    "df['target'] = housing['target']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divimos en train, test y validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data,\n",
    "                                                              housing.target)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full,\n",
    "                                                      y_train_full)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11610, 8)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Montamos el modelo. Simplemente se compondrá de una hidden layer, a la que le configuramos una capa previa de entrada de 8 neuronas (las features).\n",
    "\n",
    "Se trata de un modelo de regresión, por lo que la capa de salida es una única neurona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "362.8125"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "11610/32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8,)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.5215 - val_loss: 0.5227\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4606 - val_loss: 0.4273\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4300 - val_loss: 0.4178\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4020 - val_loss: 0.4060\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3868 - val_loss: 0.3977\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3784 - val_loss: 0.3795\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3788 - val_loss: 0.3878\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3720 - val_loss: 0.3940\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3675 - val_loss: 0.4088\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3760 - val_loss: 0.3747\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3719 - val_loss: 0.3873\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3696 - val_loss: 0.3781\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3763 - val_loss: 0.3641\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3613 - val_loss: 0.3710\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3580 - val_loss: 0.3628\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3561 - val_loss: 0.3782\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3548 - val_loss: 0.3668\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3556 - val_loss: 0.3753\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3606 - val_loss: 0.3740\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3672 - val_loss: 0.3652\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation = 'relu',\n",
    "                      input_shape = X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss = \"mean_squared_error\",\n",
    "             optimizer = \"sgd\")\n",
    "\n",
    "history = model.fit(X_train,\n",
    "                   y_train,\n",
    "                   epochs = 20,\n",
    "                   validation_data = (X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "270"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8*30 + 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_6 (Dense)             (None, 30)                270       \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 31        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 301\n",
      "Trainable params: 301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 2ms/step - loss: 0.3379\n",
      "0.33788517117500305\n"
     ]
    }
   ],
   "source": [
    "mse_test = model.evaluate(X_test, y_test)\n",
    "print(mse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.2275456 ],\n",
       "       [0.9816926 ],\n",
       "       [1.491868  ],\n",
       "       [0.36138475],\n",
       "       [1.066329  ]], dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test[:5])\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar modelo\n",
    "Para guardar el modelo, en el formato de Keras (HDF5). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_keras_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lo volvemos a cargar\n",
    "model = keras.models.load_model(\"my_keras_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks\n",
    "Son funciones predefinidas de Keras a aplicar durante el entrenamiento\n",
    "Por ejemplo, `ModelCheckpoint` sirve para que el modelo se vaya guardando tras cada epoch. Así no perdemos el progreso en caso de que decidamos interrumpir el entrenamiento. El callback recibe como argumento el nombre del objeto donde queremos que se guarde el modelo entrenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3507\n",
      "Epoch 2/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3529\n",
      "Epoch 3/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3469A: 0s - loss:\n",
      "Epoch 4/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3500\n",
      "Epoch 5/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3428\n",
      "Epoch 6/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3414\n",
      "Epoch 7/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3423\n",
      "Epoch 8/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3406\n",
      "Epoch 9/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3389\n",
      "Epoch 10/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3396\n",
      "Epoch 11/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3397\n",
      "Epoch 12/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3378\n",
      "Epoch 13/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3365\n",
      "Epoch 14/30\n",
      "360/363 [============================>.] - ETA: 0s - loss: 0.3360"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\MIGUEL~1\\AppData\\Local\\Temp/ipykernel_14068/577377571.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m                    \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                    \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m                    callbacks = [checkpoint_cb])\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Miguel Angel\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Miguel Angel\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1206\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1207\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1208\u001b[1;33m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1209\u001b[0m             with tf.profiler.experimental.Trace(\n\u001b[0;32m   1210\u001b[0m                 \u001b[1;34m'train'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Miguel Angel\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36msteps\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1248\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1249\u001b[0m         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1250\u001b[1;33m       \u001b[0moriginal_spe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_steps_per_execution\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1251\u001b[0m       can_run_full_execution = (\n\u001b[0;32m   1252\u001b[0m           \u001b[0moriginal_spe\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Miguel Angel\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    643\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 645\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    646\u001b[0m     raise NotImplementedError(\n\u001b[0;32m    647\u001b[0m         \"numpy() is only available when eager execution is enabled.\")\n",
      "\u001b[1;32mc:\\Users\\Miguel Angel\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36mread_value\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    718\u001b[0m     \"\"\"\n\u001b[0;32m    719\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Read\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 720\u001b[1;33m       \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_variable_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    721\u001b[0m     \u001b[1;31m# Return an identity so it can get placed on whatever device the context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    722\u001b[0m     \u001b[1;31m# specifies instead of the device where the variable is.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Miguel Angel\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36m_read_variable_op\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    697\u001b[0m           \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_and_set_handle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_and_set_handle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    700\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Miguel Angel\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36mread_and_set_handle\u001b[1;34m()\u001b[0m\n\u001b[0;32m    688\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread_and_set_handle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    689\u001b[0m       result = gen_resource_variable_ops.read_variable_op(\n\u001b[1;32m--> 690\u001b[1;33m           self.handle, self._dtype)\n\u001b[0m\u001b[0;32m    691\u001b[0m       \u001b[0m_maybe_set_handle_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    692\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Miguel Angel\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py\u001b[0m in \u001b[0;36mread_variable_op\u001b[1;34m(resource, dtype, name)\u001b[0m\n\u001b[0;32m    468\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    469\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[1;32m--> 470\u001b[1;33m         _ctx, \"ReadVariableOp\", name, resource, \"dtype\", dtype)\n\u001b[0m\u001b[0;32m    471\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    472\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"callback_model.h5\")\n",
    "history = model.fit(X_train,\n",
    "                   y_train,\n",
    "                   epochs=30,\n",
    "                   callbacks = [checkpoint_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stopping\n",
    "Interrumpe el entrenamiento cuando no ve progreso en el set de validación. Para ello tiene en cuenta un numero de epochs llamado `patience`. Se puede combinar con el callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3350 - val_loss: 0.3488\n",
      "Epoch 2/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3495 - val_loss: 0.3649\n",
      "Epoch 3/50\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3386 - val_loss: 0.3442\n",
      "Epoch 4/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3366 - val_loss: 0.3457\n",
      "Epoch 5/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3333 - val_loss: 0.3457\n",
      "Epoch 6/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3307 - val_loss: 0.3456\n",
      "Epoch 7/50\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3297 - val_loss: 0.3460\n",
      "Epoch 8/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3296 - val_loss: 0.3454\n"
     ]
    }
   ],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=5)\n",
    "history = model.fit(X_train,\n",
    "                   y_train,\n",
    "                   epochs=50,\n",
    "                   validation_data = (X_valid, y_valid),\n",
    "                   callbacks = [early_stopping_cb])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "90139cb9a825bf3d63f6f6704e828dbd1ff7edbd4d0c6e906a71235d6efc74af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
