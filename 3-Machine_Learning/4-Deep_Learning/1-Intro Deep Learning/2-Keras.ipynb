{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras\n",
    "Librería para programar redes neuronales de una manera más sencilla que con TensorFlow. Keras se encuentra en una capa de abstracción por encima de TensorFlow.\n",
    "\n",
    "[Documentación](https://keras.io/guides/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow\n",
    "# !pip install keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empezamos importando librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos los datos de mnist. No vamos a tratar imagenes con redes convolucionales (perdemos la estructura espacial 2D). Todos los pixeles se convertirán en un vector de 28x28 features independientes, que serán las entradas del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cogemos las imágenes de los dígitos asi como el conjunto de train y test\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos dimensiones del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "60.000 imagenes de 28x28 pixeles\n",
    "'''\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
       "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
       "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
       "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
       "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
       "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
       "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
       "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
       "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
       "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
       "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
       "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
       "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
       "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "60.000 imágenes de 28x28 pixeles. Vamos a representar una de ellas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(X_train[0], cmap=plt.cm.get_cmap('Greys'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada imagen se compone de 28x28 pixeles, y cada pixel representa una escala de grises que va del 0 al 255. Siendo 0 el blanco y 255 negro.\n",
    "\n",
    "¿Se te ocurre alguna manera de normalizar los datos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5019607843137255"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "128/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "255/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(\"float32\")/255\n",
    "X_test = X_test.astype(\"float32\")/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13066062"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.01176471, 0.07058824, 0.07058824,\n",
       "        0.07058824, 0.49411765, 0.53333336, 0.6862745 , 0.10196079,\n",
       "        0.6509804 , 1.        , 0.96862745, 0.49803922, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.11764706, 0.14117648,\n",
       "        0.36862746, 0.6039216 , 0.6666667 , 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.88235295, 0.6745098 ,\n",
       "        0.99215686, 0.9490196 , 0.7647059 , 0.2509804 , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.19215687, 0.93333334, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.9843137 , 0.3647059 , 0.32156864,\n",
       "        0.32156864, 0.21960784, 0.15294118, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.07058824, 0.85882354, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.7764706 ,\n",
       "        0.7137255 , 0.96862745, 0.94509804, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.3137255 , 0.6117647 ,\n",
       "        0.41960785, 0.99215686, 0.99215686, 0.8039216 , 0.04313726,\n",
       "        0.        , 0.16862746, 0.6039216 , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.05490196,\n",
       "        0.00392157, 0.6039216 , 0.99215686, 0.3529412 , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.54509807, 0.99215686, 0.74509805, 0.00784314,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.04313726, 0.74509805, 0.99215686, 0.27450982,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.13725491, 0.94509804, 0.88235295,\n",
       "        0.627451  , 0.42352942, 0.00392157, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.31764707, 0.9411765 ,\n",
       "        0.99215686, 0.99215686, 0.46666667, 0.09803922, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.1764706 ,\n",
       "        0.7294118 , 0.99215686, 0.99215686, 0.5882353 , 0.10588235,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.0627451 , 0.3647059 , 0.9882353 , 0.99215686, 0.73333335,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.9764706 , 0.99215686, 0.9764706 ,\n",
       "        0.2509804 , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.18039216,\n",
       "        0.50980395, 0.7176471 , 0.99215686, 0.99215686, 0.8117647 ,\n",
       "        0.00784314, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.15294118, 0.5803922 , 0.8980392 ,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.98039216, 0.7137255 ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.09411765, 0.44705883, 0.8666667 , 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.7882353 , 0.30588236, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.09019608, 0.25882354,\n",
       "        0.8352941 , 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "        0.7764706 , 0.31764707, 0.00784314, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.07058824, 0.67058825, 0.85882354, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.7647059 , 0.3137255 ,\n",
       "        0.03529412, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.21568628,\n",
       "        0.6745098 , 0.8862745 , 0.99215686, 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.95686275, 0.52156866, 0.04313726, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.53333336,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.83137256, 0.5294118 ,\n",
       "        0.5176471 , 0.0627451 , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Comprobamos la normalización\n",
    "'''\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.astype(\"float32\")\n",
    "y_test = y_test.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardamos datos para validación. Estos datos se usarán durante el entrenamiento. Otra opción es decirle a keras en la etapa de entrenamiento que reserve un X % de los datos para validar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = X_train[-10000:]\n",
    "y_val = y_train[-10000:]\n",
    "\n",
    "X_train = X_train[:-10000]\n",
    "y_train = y_train[:-10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 28, 28)\n",
      "(10000, 28, 28)\n",
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Montamos la arquitectura de la red neuronal. Se va a componer de:\n",
    "* **Sequential**: API para iniciar la red neuronal. No cuenta como capa.\n",
    "* **Flatten**: capa de entrada. Necesita un vector unidimensional. Como tenemos imágenes, esta capa aplana las imagenes (2D) en 1D.\n",
    "* **Dense**: es una hidden layer. Se compondrá de `n` neuronas y de una función de activación que se aplicará a todas las neuronas de la capa.\n",
    "\n",
    "Recuerda que es un problema de clasificación multiclase (10 clases) y que por tanto la última capa se compondrá de tantas neuronas como clases tengas.\n",
    "\n",
    "En cuanto a las funciones de activación es recomendable usar relu en las hidden layer, que tarda menos en entrenar, mientras que la ultima (output) suele ser una softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "28*28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "\n",
    "# Capa entrada\n",
    "model.add(keras.layers.Flatten(input_shape=(28, 28)))\n",
    "\n",
    "# Hidden layer\n",
    "model.add(keras.layers.Dense(units = 300,\n",
    "                            activation='relu'))\n",
    "\n",
    "# Hidden layer\n",
    "model.add(keras.layers.Dense(units = 100,\n",
    "                            activation='relu'))\n",
    "\n",
    "# Capa salida\n",
    "model.add(keras.layers.Dense(units = 10,\n",
    "                            activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Otra manera de declarar la red neuronal\n",
    "capas = [\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(units = 300, activation='relu'),\n",
    "    keras.layers.Dense(units = 100, activation='relu'),\n",
    "    keras.layers.Dense(units = 10, activation='softmax')\n",
    "]\n",
    "\n",
    "model = keras.models.Sequential(capas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver las capas, y acceder a sus elementos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.layers.core.flatten.Flatten object at 0x000001F848D27A48>\n"
     ]
    }
   ],
   "source": [
    "print(model.layers[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver los pesos de las capas sin entrenar, porque los inicializa aleatoriamente. Los bias los inicializa a 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden1 = model.layers[1]\n",
    "weights, biases = hidden1.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.06619284, -0.02426902, -0.03235549, ...,  0.0377613 ,\n",
       "         0.047301  ,  0.00100458],\n",
       "       [ 0.00381584, -0.0413104 ,  0.04221852, ..., -0.06347035,\n",
       "         0.04193456, -0.02041588],\n",
       "       [-0.07132974, -0.05726838, -0.0410872 , ...,  0.0432393 ,\n",
       "         0.06311813,  0.06114404],\n",
       "       ...,\n",
       "       [-0.0088836 ,  0.00322781,  0.01062436, ..., -0.00482301,\n",
       "        -0.0533157 ,  0.01518884],\n",
       "       [-0.03160513, -0.05940086,  0.03161184, ..., -0.05553841,\n",
       "        -0.01560372,  0.05393505],\n",
       "       [-0.05069605, -0.00480857, -0.06123217, ..., -0.04895335,\n",
       "        -0.0134495 , -0.0621233 ]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(weights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "28*28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235200"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "300*784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235200"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establecemos la configuración de ejecución... el compile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = keras.optimizers.SGD(),\n",
    "    loss = keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics = [keras.metrics.SparseCategoricalAccuracy()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equivalente\n",
    "model.compile(\n",
    "    optimizer = \"sgd\",\n",
    "    loss = \"sparse_categorical_crossentropy\",\n",
    "    metrics = [\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_2 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 300)               235500    \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 100)               30100     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235500\n"
     ]
    }
   ],
   "source": [
    "# 1º neurona de la 1º hidden layer\n",
    "# y = a + w1*x1 + w2*x2 + .... wn*xn\n",
    "# a es el intercepto llamado bias\n",
    "# wn es cada uno de los pesos que va a ir actualizando con el backpropagation\n",
    "# n es 784\n",
    "# En la 1º hidden layer tenemos 784 pesos por cada neurona, al tener 300, tenemos un total de:\n",
    "print(784*300 + 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235500"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "300 * 784 + 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30100"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "300 * 100 + 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1010"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100 * 10 + 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos el modelo. Usamos los datos de entrenamiento. El batch_size es la cantidad de muestras que utiliza el SGD, y las epochs son las iteraciones que realiza en el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 28, 28)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "390.625"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "50000/128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "391/391 [==============================] - 5s 9ms/step - loss: 1.2995 - accuracy: 0.6775 - val_loss: 0.6319 - val_accuracy: 0.8652\n",
      "Epoch 2/50\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.5351 - accuracy: 0.8673 - val_loss: 0.4088 - val_accuracy: 0.8935\n",
      "Epoch 3/50\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4091 - accuracy: 0.8892 - val_loss: 0.3446 - val_accuracy: 0.9049\n",
      "Epoch 4/50\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3582 - accuracy: 0.9005 - val_loss: 0.3122 - val_accuracy: 0.9133\n",
      "Epoch 5/50\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3273 - accuracy: 0.9081 - val_loss: 0.2883 - val_accuracy: 0.9184\n",
      "Epoch 6/50\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3049 - accuracy: 0.9144 - val_loss: 0.2728 - val_accuracy: 0.9204\n",
      "Epoch 7/50\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2874 - accuracy: 0.9190 - val_loss: 0.2588 - val_accuracy: 0.9252\n",
      "Epoch 8/50\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.2721 - accuracy: 0.9236 - val_loss: 0.2474 - val_accuracy: 0.9283\n",
      "Epoch 9/50\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2592 - accuracy: 0.9270 - val_loss: 0.2369 - val_accuracy: 0.9323\n",
      "Epoch 10/50\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2478 - accuracy: 0.9299 - val_loss: 0.2275 - val_accuracy: 0.9350\n",
      "Epoch 11/50\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2375 - accuracy: 0.9330 - val_loss: 0.2191 - val_accuracy: 0.9373\n",
      "Epoch 12/50\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2280 - accuracy: 0.9355 - val_loss: 0.2123 - val_accuracy: 0.9399\n",
      "Epoch 13/50\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2192 - accuracy: 0.9374 - val_loss: 0.2058 - val_accuracy: 0.9402\n",
      "Epoch 14/50\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2116 - accuracy: 0.9401 - val_loss: 0.1987 - val_accuracy: 0.9443\n",
      "Epoch 15/50\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2041 - accuracy: 0.9420 - val_loss: 0.1931 - val_accuracy: 0.9457\n",
      "Epoch 16/50\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.1971 - accuracy: 0.9444 - val_loss: 0.1878 - val_accuracy: 0.9474\n",
      "Epoch 17/50\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.1906 - accuracy: 0.9458 - val_loss: 0.1824 - val_accuracy: 0.9497\n",
      "Epoch 18/50\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.1846 - accuracy: 0.9471 - val_loss: 0.1776 - val_accuracy: 0.9516\n",
      "Epoch 19/50\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1789 - accuracy: 0.9490 - val_loss: 0.1740 - val_accuracy: 0.9521\n",
      "Epoch 20/50\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1734 - accuracy: 0.9506 - val_loss: 0.1692 - val_accuracy: 0.9546\n",
      "Epoch 21/50\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.1685 - accuracy: 0.9516 - val_loss: 0.1653 - val_accuracy: 0.9562\n",
      "Epoch 22/50\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1635 - accuracy: 0.9533 - val_loss: 0.1613 - val_accuracy: 0.9560\n",
      "Epoch 23/50\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1588 - accuracy: 0.9552 - val_loss: 0.1574 - val_accuracy: 0.9569\n",
      "Epoch 24/50\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1545 - accuracy: 0.9561 - val_loss: 0.1545 - val_accuracy: 0.9579\n",
      "Epoch 25/50\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1502 - accuracy: 0.9569 - val_loss: 0.1525 - val_accuracy: 0.9575\n",
      "Epoch 26/50\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1463 - accuracy: 0.9584 - val_loss: 0.1486 - val_accuracy: 0.9590\n",
      "Epoch 27/50\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1426 - accuracy: 0.9595 - val_loss: 0.1462 - val_accuracy: 0.9604\n",
      "Epoch 28/50\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1388 - accuracy: 0.9608 - val_loss: 0.1433 - val_accuracy: 0.9611\n",
      "Epoch 29/50\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1356 - accuracy: 0.9617 - val_loss: 0.1407 - val_accuracy: 0.9616\n",
      "Epoch 30/50\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1320 - accuracy: 0.9625 - val_loss: 0.1380 - val_accuracy: 0.9620\n",
      "Epoch 31/50\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.1289 - accuracy: 0.9634 - val_loss: 0.1364 - val_accuracy: 0.9629\n",
      "Epoch 32/50\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1258 - accuracy: 0.9649 - val_loss: 0.1338 - val_accuracy: 0.9638\n",
      "Epoch 33/50\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1229 - accuracy: 0.9652 - val_loss: 0.1320 - val_accuracy: 0.9632\n",
      "Epoch 34/50\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1200 - accuracy: 0.9663 - val_loss: 0.1286 - val_accuracy: 0.9644\n",
      "Epoch 35/50\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1174 - accuracy: 0.9673 - val_loss: 0.1268 - val_accuracy: 0.9652\n",
      "Epoch 36/50\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1147 - accuracy: 0.9680 - val_loss: 0.1256 - val_accuracy: 0.9655\n",
      "Epoch 37/50\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.1123 - accuracy: 0.9691 - val_loss: 0.1243 - val_accuracy: 0.9656\n",
      "Epoch 38/50\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.1098 - accuracy: 0.9697 - val_loss: 0.1221 - val_accuracy: 0.9665\n",
      "Epoch 39/50\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1074 - accuracy: 0.9701 - val_loss: 0.1208 - val_accuracy: 0.9667\n",
      "Epoch 40/50\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1050 - accuracy: 0.9709 - val_loss: 0.1221 - val_accuracy: 0.9663\n",
      "Epoch 41/50\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1029 - accuracy: 0.9716 - val_loss: 0.1181 - val_accuracy: 0.9682\n",
      "Epoch 42/50\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.1009 - accuracy: 0.9722 - val_loss: 0.1176 - val_accuracy: 0.9674\n",
      "Epoch 43/50\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.0989 - accuracy: 0.9730 - val_loss: 0.1146 - val_accuracy: 0.9696\n",
      "Epoch 44/50\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.0968 - accuracy: 0.9734 - val_loss: 0.1137 - val_accuracy: 0.9690\n",
      "Epoch 45/50\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.0949 - accuracy: 0.9742 - val_loss: 0.1129 - val_accuracy: 0.9694\n",
      "Epoch 46/50\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.0930 - accuracy: 0.9747 - val_loss: 0.1115 - val_accuracy: 0.9692\n",
      "Epoch 47/50\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.0913 - accuracy: 0.9749 - val_loss: 0.1096 - val_accuracy: 0.9697\n",
      "Epoch 48/50\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.0895 - accuracy: 0.9760 - val_loss: 0.1092 - val_accuracy: 0.9698\n",
      "Epoch 49/50\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.0877 - accuracy: 0.9763 - val_loss: 0.1078 - val_accuracy: 0.9697\n",
      "Epoch 50/50\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.0861 - accuracy: 0.9775 - val_loss: 0.1077 - val_accuracy: 0.9691\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size = 128,\n",
    "    epochs = 50,\n",
    "    validation_data = (X_val, y_val) # validation_split = 0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos reentrenar el modelo. No empieza de nuevo, sino que retoma el entrenamiento anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 0.0855 - accuracy: 0.9769 - val_loss: 0.1078 - val_accuracy: 0.9706\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 0.0825 - accuracy: 0.9778 - val_loss: 0.1030 - val_accuracy: 0.9703\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.0796 - accuracy: 0.9786 - val_loss: 0.1028 - val_accuracy: 0.9706\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 0.0768 - accuracy: 0.9794 - val_loss: 0.0997 - val_accuracy: 0.9710\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0743 - accuracy: 0.9804 - val_loss: 0.1013 - val_accuracy: 0.9718\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0717 - accuracy: 0.9810 - val_loss: 0.1009 - val_accuracy: 0.9719\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.0696 - accuracy: 0.9815 - val_loss: 0.0957 - val_accuracy: 0.9731\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.0671 - accuracy: 0.9822 - val_loss: 0.0947 - val_accuracy: 0.9729\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 0.0651 - accuracy: 0.9822 - val_loss: 0.0952 - val_accuracy: 0.9734\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 0.0629 - accuracy: 0.9833 - val_loss: 0.0928 - val_accuracy: 0.9738\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f849049b88>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size = 64,\n",
    "    epochs = 10,\n",
    "    validation_data = (X_val, y_val) # validation_split = 0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos el histórico del entrenamiento, para poder representarlo posteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': [1.299493670463562, 0.5351290702819824, 0.4091475009918213, 0.3581938147544861, 0.3273318111896515, 0.3048820197582245, 0.2873517572879791, 0.27209362387657166, 0.25923413038253784, 0.24775199592113495, 0.23751860857009888, 0.22804492712020874, 0.21919144690036774, 0.2115759402513504, 0.20414260029792786, 0.19714337587356567, 0.19056548178195953, 0.1845857948064804, 0.178920716047287, 0.1734403669834137, 0.16849523782730103, 0.16349884867668152, 0.1588316708803177, 0.1544949859380722, 0.15017060935497284, 0.14634937047958374, 0.14259998500347137, 0.13878610730171204, 0.13560384511947632, 0.1320042908191681, 0.12889741361141205, 0.12578439712524414, 0.12286099791526794, 0.12004081904888153, 0.11744091659784317, 0.11474618315696716, 0.11225536465644836, 0.10976525396108627, 0.10742879658937454, 0.10499207675457001, 0.10292021185159683, 0.1008518785238266, 0.0989055186510086, 0.09678953886032104, 0.0948995053768158, 0.09299398213624954, 0.09129103273153305, 0.08948370814323425, 0.08769133687019348, 0.08612240105867386], 'accuracy': [0.6774799823760986, 0.8672800064086914, 0.8892199993133545, 0.9005399942398071, 0.9080600142478943, 0.914359986782074, 0.9189599752426147, 0.9235799908638, 0.9269999861717224, 0.9299399852752686, 0.9330199956893921, 0.9355400204658508, 0.9373999834060669, 0.9401000142097473, 0.9419599771499634, 0.944379985332489, 0.9457600116729736, 0.9471399784088135, 0.94896000623703, 0.9505599737167358, 0.9516199827194214, 0.9533200263977051, 0.9551799893379211, 0.9560800194740295, 0.9568799734115601, 0.9584400057792664, 0.9595400094985962, 0.9608399868011475, 0.9617199897766113, 0.9624599814414978, 0.9634199738502502, 0.9648600220680237, 0.9651600122451782, 0.9663199782371521, 0.9673200249671936, 0.9680399894714355, 0.9690600037574768, 0.9696999788284302, 0.9701200127601624, 0.9708600044250488, 0.9715999960899353, 0.9721599817276001, 0.9730200171470642, 0.9734200239181519, 0.9741799831390381, 0.9746999740600586, 0.9749000072479248, 0.9760000109672546, 0.9763399958610535, 0.9775199890136719], 'val_loss': [0.6319083571434021, 0.4088301658630371, 0.3445988595485687, 0.31224918365478516, 0.288327693939209, 0.27275994420051575, 0.2588285803794861, 0.24737949669361115, 0.23688650131225586, 0.22748345136642456, 0.21914057433605194, 0.21228379011154175, 0.2058294415473938, 0.1987147182226181, 0.19311410188674927, 0.18778426945209503, 0.18238559365272522, 0.1775779128074646, 0.17401570081710815, 0.1691560596227646, 0.1652882844209671, 0.16125468909740448, 0.15741628408432007, 0.15450036525726318, 0.1525438129901886, 0.14864762127399445, 0.14618642628192902, 0.14327040314674377, 0.1406659185886383, 0.13796764612197876, 0.13637228310108185, 0.13382333517074585, 0.13204741477966309, 0.12864162027835846, 0.12681113183498383, 0.1256008744239807, 0.1243031769990921, 0.1220879852771759, 0.12082087248563766, 0.12205815315246582, 0.11814457923173904, 0.1176137700676918, 0.11458101868629456, 0.11368753761053085, 0.1128794401884079, 0.11147663742303848, 0.10963305830955505, 0.10919805616140366, 0.1078108549118042, 0.1076582670211792], 'val_accuracy': [0.8651999831199646, 0.8934999704360962, 0.9049000144004822, 0.9132999777793884, 0.91839998960495, 0.9204000234603882, 0.9251999855041504, 0.9283000230789185, 0.9322999715805054, 0.9350000023841858, 0.9373000264167786, 0.9398999810218811, 0.9401999711990356, 0.9442999958992004, 0.9456999897956848, 0.9473999738693237, 0.9496999979019165, 0.9516000151634216, 0.9520999789237976, 0.9545999765396118, 0.9562000036239624, 0.9559999704360962, 0.9569000005722046, 0.9578999876976013, 0.9574999809265137, 0.9589999914169312, 0.9603999853134155, 0.9610999822616577, 0.9616000056266785, 0.9620000123977661, 0.9628999829292297, 0.9638000130653381, 0.9631999731063843, 0.9643999934196472, 0.9652000069618225, 0.965499997138977, 0.9656000137329102, 0.9664999842643738, 0.96670001745224, 0.9663000106811523, 0.9682000279426575, 0.9674000144004822, 0.9696000218391418, 0.968999981880188, 0.9693999886512756, 0.9692000150680542, 0.9696999788284302, 0.9697999954223633, 0.9696999788284302, 0.9690999984741211]}\n"
     ]
    }
   ],
   "source": [
    "# print(history.params)\n",
    "# print(history.epoch)\n",
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.06225531920790672,\n",
       "  0.06138444319367409,\n",
       "  0.06027477979660034,\n",
       "  0.059108540415763855,\n",
       "  0.058218587189912796,\n",
       "  0.05721678584814072,\n",
       "  0.05623963847756386,\n",
       "  0.055387239903211594,\n",
       "  0.05441601574420929,\n",
       "  0.05352767929434776,\n",
       "  0.052592288702726364,\n",
       "  0.051786866039037704,\n",
       "  0.05093264952301979,\n",
       "  0.05003897473216057,\n",
       "  0.0492832325398922,\n",
       "  0.04847511276602745,\n",
       "  0.04775787517428398,\n",
       "  0.04693999141454697,\n",
       "  0.046214886009693146,\n",
       "  0.045556679368019104,\n",
       "  0.04481605067849159,\n",
       "  0.04405045881867409,\n",
       "  0.04339233413338661,\n",
       "  0.04278245195746422,\n",
       "  0.042025357484817505,\n",
       "  0.041410598903894424,\n",
       "  0.040810007601976395,\n",
       "  0.04007289931178093,\n",
       "  0.03942100703716278,\n",
       "  0.03892222046852112],\n",
       " 'accuracy': [0.9837200045585632,\n",
       "  0.9840800166130066,\n",
       "  0.983959972858429,\n",
       "  0.9847999811172485,\n",
       "  0.9847999811172485,\n",
       "  0.9850999712944031,\n",
       "  0.9853399991989136,\n",
       "  0.9857400059700012,\n",
       "  0.986020028591156,\n",
       "  0.9861599802970886,\n",
       "  0.9864799976348877,\n",
       "  0.9866200089454651,\n",
       "  0.9870399832725525,\n",
       "  0.987280011177063,\n",
       "  0.9876800179481506,\n",
       "  0.9876800179481506,\n",
       "  0.987779974937439,\n",
       "  0.9884399771690369,\n",
       "  0.9884399771690369,\n",
       "  0.9886199831962585,\n",
       "  0.9889600276947021,\n",
       "  0.9894400238990784,\n",
       "  0.9895399808883667,\n",
       "  0.9896399974822998,\n",
       "  0.9898200035095215,\n",
       "  0.989799976348877,\n",
       "  0.9901400208473206,\n",
       "  0.9903799891471863,\n",
       "  0.9907799959182739,\n",
       "  0.9908999800682068],\n",
       " 'val_loss': [0.09174215793609619,\n",
       "  0.09083504974842072,\n",
       "  0.09042980521917343,\n",
       "  0.0904356986284256,\n",
       "  0.0902385339140892,\n",
       "  0.08940856158733368,\n",
       "  0.08886448293924332,\n",
       "  0.08740954101085663,\n",
       "  0.08680478483438492,\n",
       "  0.0871112123131752,\n",
       "  0.08641093969345093,\n",
       "  0.08738430589437485,\n",
       "  0.08585700392723083,\n",
       "  0.0853617712855339,\n",
       "  0.08460068702697754,\n",
       "  0.08507869392633438,\n",
       "  0.08454790711402893,\n",
       "  0.08406270295381546,\n",
       "  0.08393722772598267,\n",
       "  0.08294570446014404,\n",
       "  0.0827503651380539,\n",
       "  0.08226973563432693,\n",
       "  0.08175982534885406,\n",
       "  0.08106722682714462,\n",
       "  0.08092717826366425,\n",
       "  0.08086879551410675,\n",
       "  0.0810677707195282,\n",
       "  0.07988576591014862,\n",
       "  0.07975463569164276,\n",
       "  0.0791693851351738],\n",
       " 'val_accuracy': [0.972000002861023,\n",
       "  0.9731000065803528,\n",
       "  0.9732999801635742,\n",
       "  0.9732999801635742,\n",
       "  0.9733999967575073,\n",
       "  0.9736999869346619,\n",
       "  0.9732000231742859,\n",
       "  0.9736999869346619,\n",
       "  0.9746999740600586,\n",
       "  0.9735999703407288,\n",
       "  0.974399983882904,\n",
       "  0.9745000004768372,\n",
       "  0.9745000004768372,\n",
       "  0.9743000268936157,\n",
       "  0.9753999710083008,\n",
       "  0.9753999710083008,\n",
       "  0.9750999808311462,\n",
       "  0.9746000170707703,\n",
       "  0.9758999943733215,\n",
       "  0.9751999974250793,\n",
       "  0.975600004196167,\n",
       "  0.9764000177383423,\n",
       "  0.9764000177383423,\n",
       "  0.9761999845504761,\n",
       "  0.9761999845504761,\n",
       "  0.9771999716758728,\n",
       "  0.9765999913215637,\n",
       "  0.9767000079154968,\n",
       "  0.9765999913215637,\n",
       "  0.9778000116348267]}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.299494</td>\n",
       "      <td>0.67748</td>\n",
       "      <td>0.631908</td>\n",
       "      <td>0.8652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.535129</td>\n",
       "      <td>0.86728</td>\n",
       "      <td>0.408830</td>\n",
       "      <td>0.8935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.409148</td>\n",
       "      <td>0.88922</td>\n",
       "      <td>0.344599</td>\n",
       "      <td>0.9049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.358194</td>\n",
       "      <td>0.90054</td>\n",
       "      <td>0.312249</td>\n",
       "      <td>0.9133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.327332</td>\n",
       "      <td>0.90806</td>\n",
       "      <td>0.288328</td>\n",
       "      <td>0.9184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.304882</td>\n",
       "      <td>0.91436</td>\n",
       "      <td>0.272760</td>\n",
       "      <td>0.9204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.287352</td>\n",
       "      <td>0.91896</td>\n",
       "      <td>0.258829</td>\n",
       "      <td>0.9252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.272094</td>\n",
       "      <td>0.92358</td>\n",
       "      <td>0.247379</td>\n",
       "      <td>0.9283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.259234</td>\n",
       "      <td>0.92700</td>\n",
       "      <td>0.236887</td>\n",
       "      <td>0.9323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.247752</td>\n",
       "      <td>0.92994</td>\n",
       "      <td>0.227483</td>\n",
       "      <td>0.9350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.237519</td>\n",
       "      <td>0.93302</td>\n",
       "      <td>0.219141</td>\n",
       "      <td>0.9373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.228045</td>\n",
       "      <td>0.93554</td>\n",
       "      <td>0.212284</td>\n",
       "      <td>0.9399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.219191</td>\n",
       "      <td>0.93740</td>\n",
       "      <td>0.205829</td>\n",
       "      <td>0.9402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.211576</td>\n",
       "      <td>0.94010</td>\n",
       "      <td>0.198715</td>\n",
       "      <td>0.9443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.204143</td>\n",
       "      <td>0.94196</td>\n",
       "      <td>0.193114</td>\n",
       "      <td>0.9457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.197143</td>\n",
       "      <td>0.94438</td>\n",
       "      <td>0.187784</td>\n",
       "      <td>0.9474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.190565</td>\n",
       "      <td>0.94576</td>\n",
       "      <td>0.182386</td>\n",
       "      <td>0.9497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.184586</td>\n",
       "      <td>0.94714</td>\n",
       "      <td>0.177578</td>\n",
       "      <td>0.9516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.178921</td>\n",
       "      <td>0.94896</td>\n",
       "      <td>0.174016</td>\n",
       "      <td>0.9521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.173440</td>\n",
       "      <td>0.95056</td>\n",
       "      <td>0.169156</td>\n",
       "      <td>0.9546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.168495</td>\n",
       "      <td>0.95162</td>\n",
       "      <td>0.165288</td>\n",
       "      <td>0.9562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.163499</td>\n",
       "      <td>0.95332</td>\n",
       "      <td>0.161255</td>\n",
       "      <td>0.9560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.158832</td>\n",
       "      <td>0.95518</td>\n",
       "      <td>0.157416</td>\n",
       "      <td>0.9569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.154495</td>\n",
       "      <td>0.95608</td>\n",
       "      <td>0.154500</td>\n",
       "      <td>0.9579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.150171</td>\n",
       "      <td>0.95688</td>\n",
       "      <td>0.152544</td>\n",
       "      <td>0.9575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.146349</td>\n",
       "      <td>0.95844</td>\n",
       "      <td>0.148648</td>\n",
       "      <td>0.9590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.142600</td>\n",
       "      <td>0.95954</td>\n",
       "      <td>0.146186</td>\n",
       "      <td>0.9604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.138786</td>\n",
       "      <td>0.96084</td>\n",
       "      <td>0.143270</td>\n",
       "      <td>0.9611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.135604</td>\n",
       "      <td>0.96172</td>\n",
       "      <td>0.140666</td>\n",
       "      <td>0.9616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.132004</td>\n",
       "      <td>0.96246</td>\n",
       "      <td>0.137968</td>\n",
       "      <td>0.9620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.128897</td>\n",
       "      <td>0.96342</td>\n",
       "      <td>0.136372</td>\n",
       "      <td>0.9629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.125784</td>\n",
       "      <td>0.96486</td>\n",
       "      <td>0.133823</td>\n",
       "      <td>0.9638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.122861</td>\n",
       "      <td>0.96516</td>\n",
       "      <td>0.132047</td>\n",
       "      <td>0.9632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.120041</td>\n",
       "      <td>0.96632</td>\n",
       "      <td>0.128642</td>\n",
       "      <td>0.9644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.117441</td>\n",
       "      <td>0.96732</td>\n",
       "      <td>0.126811</td>\n",
       "      <td>0.9652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.114746</td>\n",
       "      <td>0.96804</td>\n",
       "      <td>0.125601</td>\n",
       "      <td>0.9655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.112255</td>\n",
       "      <td>0.96906</td>\n",
       "      <td>0.124303</td>\n",
       "      <td>0.9656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.109765</td>\n",
       "      <td>0.96970</td>\n",
       "      <td>0.122088</td>\n",
       "      <td>0.9665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.107429</td>\n",
       "      <td>0.97012</td>\n",
       "      <td>0.120821</td>\n",
       "      <td>0.9667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.104992</td>\n",
       "      <td>0.97086</td>\n",
       "      <td>0.122058</td>\n",
       "      <td>0.9663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.102920</td>\n",
       "      <td>0.97160</td>\n",
       "      <td>0.118145</td>\n",
       "      <td>0.9682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.100852</td>\n",
       "      <td>0.97216</td>\n",
       "      <td>0.117614</td>\n",
       "      <td>0.9674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.098906</td>\n",
       "      <td>0.97302</td>\n",
       "      <td>0.114581</td>\n",
       "      <td>0.9696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.096790</td>\n",
       "      <td>0.97342</td>\n",
       "      <td>0.113688</td>\n",
       "      <td>0.9690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.094900</td>\n",
       "      <td>0.97418</td>\n",
       "      <td>0.112879</td>\n",
       "      <td>0.9694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.092994</td>\n",
       "      <td>0.97470</td>\n",
       "      <td>0.111477</td>\n",
       "      <td>0.9692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.091291</td>\n",
       "      <td>0.97490</td>\n",
       "      <td>0.109633</td>\n",
       "      <td>0.9697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.089484</td>\n",
       "      <td>0.97600</td>\n",
       "      <td>0.109198</td>\n",
       "      <td>0.9698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.087691</td>\n",
       "      <td>0.97634</td>\n",
       "      <td>0.107811</td>\n",
       "      <td>0.9697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.086122</td>\n",
       "      <td>0.97752</td>\n",
       "      <td>0.107658</td>\n",
       "      <td>0.9691</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        loss  accuracy  val_loss  val_accuracy\n",
       "0   1.299494   0.67748  0.631908        0.8652\n",
       "1   0.535129   0.86728  0.408830        0.8935\n",
       "2   0.409148   0.88922  0.344599        0.9049\n",
       "3   0.358194   0.90054  0.312249        0.9133\n",
       "4   0.327332   0.90806  0.288328        0.9184\n",
       "5   0.304882   0.91436  0.272760        0.9204\n",
       "6   0.287352   0.91896  0.258829        0.9252\n",
       "7   0.272094   0.92358  0.247379        0.9283\n",
       "8   0.259234   0.92700  0.236887        0.9323\n",
       "9   0.247752   0.92994  0.227483        0.9350\n",
       "10  0.237519   0.93302  0.219141        0.9373\n",
       "11  0.228045   0.93554  0.212284        0.9399\n",
       "12  0.219191   0.93740  0.205829        0.9402\n",
       "13  0.211576   0.94010  0.198715        0.9443\n",
       "14  0.204143   0.94196  0.193114        0.9457\n",
       "15  0.197143   0.94438  0.187784        0.9474\n",
       "16  0.190565   0.94576  0.182386        0.9497\n",
       "17  0.184586   0.94714  0.177578        0.9516\n",
       "18  0.178921   0.94896  0.174016        0.9521\n",
       "19  0.173440   0.95056  0.169156        0.9546\n",
       "20  0.168495   0.95162  0.165288        0.9562\n",
       "21  0.163499   0.95332  0.161255        0.9560\n",
       "22  0.158832   0.95518  0.157416        0.9569\n",
       "23  0.154495   0.95608  0.154500        0.9579\n",
       "24  0.150171   0.95688  0.152544        0.9575\n",
       "25  0.146349   0.95844  0.148648        0.9590\n",
       "26  0.142600   0.95954  0.146186        0.9604\n",
       "27  0.138786   0.96084  0.143270        0.9611\n",
       "28  0.135604   0.96172  0.140666        0.9616\n",
       "29  0.132004   0.96246  0.137968        0.9620\n",
       "30  0.128897   0.96342  0.136372        0.9629\n",
       "31  0.125784   0.96486  0.133823        0.9638\n",
       "32  0.122861   0.96516  0.132047        0.9632\n",
       "33  0.120041   0.96632  0.128642        0.9644\n",
       "34  0.117441   0.96732  0.126811        0.9652\n",
       "35  0.114746   0.96804  0.125601        0.9655\n",
       "36  0.112255   0.96906  0.124303        0.9656\n",
       "37  0.109765   0.96970  0.122088        0.9665\n",
       "38  0.107429   0.97012  0.120821        0.9667\n",
       "39  0.104992   0.97086  0.122058        0.9663\n",
       "40  0.102920   0.97160  0.118145        0.9682\n",
       "41  0.100852   0.97216  0.117614        0.9674\n",
       "42  0.098906   0.97302  0.114581        0.9696\n",
       "43  0.096790   0.97342  0.113688        0.9690\n",
       "44  0.094900   0.97418  0.112879        0.9694\n",
       "45  0.092994   0.97470  0.111477        0.9692\n",
       "46  0.091291   0.97490  0.109633        0.9697\n",
       "47  0.089484   0.97600  0.109198        0.9698\n",
       "48  0.087691   0.97634  0.107811        0.9697\n",
       "49  0.086122   0.97752  0.107658        0.9691"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si el modelo no ha ido bien, prueba a cambiar el learning rate, cambia de optimizador y después prueba a cambiar capas, neuronas y funciones de activación.\n",
    "\n",
    "Ya tenemos el modelo entrenado. Probémoslo con test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0883 - accuracy: 0.9731\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0882970467209816, 0.9731000065803528]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cogemos el primero\n",
    "plt.imshow(X_test[0].reshape(28,28), cmap=plt.cm.get_cmap('Greys'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.32941177, 0.7254902 , 0.62352943, 0.5921569 ,\n",
       "         0.23529412, 0.14117648, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.87058824, 0.99607843, 0.99607843, 0.99607843,\n",
       "         0.99607843, 0.94509804, 0.7764706 , 0.7764706 , 0.7764706 ,\n",
       "         0.7764706 , 0.7764706 , 0.7764706 , 0.7764706 , 0.7764706 ,\n",
       "         0.6666667 , 0.20392157, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.2627451 , 0.44705883, 0.28235295, 0.44705883,\n",
       "         0.6392157 , 0.8901961 , 0.99607843, 0.88235295, 0.99607843,\n",
       "         0.99607843, 0.99607843, 0.98039216, 0.8980392 , 0.99607843,\n",
       "         0.99607843, 0.54901963, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.06666667, 0.25882354, 0.05490196, 0.2627451 ,\n",
       "         0.2627451 , 0.2627451 , 0.23137255, 0.08235294, 0.9254902 ,\n",
       "         0.99607843, 0.41568628, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.3254902 , 0.99215686,\n",
       "         0.81960785, 0.07058824, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.08627451, 0.9137255 , 1.        ,\n",
       "         0.3254902 , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.5058824 , 0.99607843, 0.93333334,\n",
       "         0.17254902, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.23137255, 0.9764706 , 0.99607843, 0.24313726,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.52156866, 0.99607843, 0.73333335, 0.01960784,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.03529412, 0.8039216 , 0.972549  , 0.22745098, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.49411765, 0.99607843, 0.7137255 , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.29411766,\n",
       "         0.9843137 , 0.9411765 , 0.22352941, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.07450981, 0.8666667 ,\n",
       "         0.99607843, 0.6509804 , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.01176471, 0.79607844, 0.99607843,\n",
       "         0.85882354, 0.13725491, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.14901961, 0.99607843, 0.99607843,\n",
       "         0.3019608 , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.12156863, 0.8784314 , 0.99607843, 0.4509804 ,\n",
       "         0.00392157, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.52156866, 0.99607843, 0.99607843, 0.20392157,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.23921569, 0.9490196 , 0.99607843, 0.99607843, 0.20392157,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.4745098 , 0.99607843, 0.99607843, 0.85882354, 0.15686275,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.4745098 , 0.99607843, 0.8117647 , 0.07058824, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ]]], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[5.8940809e-06, 1.9128993e-07, 1.0073608e-03, 3.5517446e-03,\n",
       "        6.9939425e-08, 5.6841000e-06, 4.8485299e-11, 9.9537885e-01,\n",
       "        2.1442465e-05, 2.8683149e-05]], dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(X_test[:1])\n",
    "print(predictions.shape)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, ..., 4, 5, 6], dtype=int64)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_test[2].reshape(28,28), cmap=plt.cm.get_cmap('Greys'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problema de regresión\n",
    "Veamos un ejemplo de cómo aplicar una red neuronal de TensorFlow a un problema de regresión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "      <td>4.526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "      <td>3.585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "      <td>3.521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "\n",
       "   Longitude  target  \n",
       "0    -122.23   4.526  \n",
       "1    -122.22   3.585  \n",
       "2    -122.24   3.521  \n",
       "3    -122.25   3.413  \n",
       "4    -122.25   3.422  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargamos datos\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "df = pd.DataFrame(housing.data, columns = housing.feature_names)\n",
    "df['target'] = housing['target']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divimos en train, test y validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data,\n",
    "                                                              housing.target)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full,\n",
    "                                                      y_train_full)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11610, 8)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Montamos el modelo. Simplemente se compondrá de una hidden layer, a la que le configuramos una capa previa de entrada de 8 neuronas (las features).\n",
    "\n",
    "Se trata de un modelo de regresión, por lo que la capa de salida es una única neurona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "362.8125"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "11610/32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8,)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 2s 3ms/step - loss: 0.9727 - val_loss: 0.6402\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 2.7003 - val_loss: 0.5107\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5004 - val_loss: 0.4699\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4249 - val_loss: 0.4486\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4062 - val_loss: 0.4200\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3800 - val_loss: 0.4043\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3679 - val_loss: 0.3991\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3603 - val_loss: 0.3942\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3552 - val_loss: 0.3787\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3494 - val_loss: 0.3784\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3449 - val_loss: 0.3747\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3429 - val_loss: 0.3739\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3433 - val_loss: 0.3639\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3395 - val_loss: 0.3667\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3407 - val_loss: 0.3612\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3342 - val_loss: 0.3604\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3320 - val_loss: 0.3684\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3334 - val_loss: 0.3645\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3304 - val_loss: 0.3599\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3299 - val_loss: 0.3561\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation = 'relu',\n",
    "                      input_shape = X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss = \"mean_squared_error\",\n",
    "             optimizer = \"sgd\")\n",
    "\n",
    "history = model.fit(X_train,\n",
    "                   y_train,\n",
    "                   epochs = 20,\n",
    "                   validation_data = (X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "270"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8*30 + 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_9 (Dense)             (None, 30)                270       \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 1)                 31        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 301\n",
      "Trainable params: 301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 2ms/step - loss: 0.3663\n",
      "0.36632218956947327\n"
     ]
    }
   ],
   "source": [
    "mse_test = model.evaluate(X_test, y_test)\n",
    "print(mse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.7854888],\n",
       "       [0.8859279],\n",
       "       [3.9356465],\n",
       "       [2.695189 ],\n",
       "       [1.8540955]], dtype=float32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test[:5])\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar modelo\n",
    "Para guardar el modelo, en el formato de Keras (HDF5). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_keras_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lo volvemos a cargar\n",
    "model = keras.models.load_model(\"my_keras_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks\n",
    "Son funciones predefinidas de Keras a aplicar durante el entrenamiento\n",
    "Por ejemplo, `ModelCheckpoint` sirve para que el modelo se vaya guardando tras cada epoch. Así no perdemos el progreso en caso de que decidamos interrumpir el entrenamiento. El callback recibe como argumento el nombre del objeto donde queremos que se guarde el modelo entrenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "363/363 [==============================] - 2s 3ms/step - loss: 0.3288\n",
      "Epoch 2/30\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3278\n",
      "Epoch 3/30\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3265A: 0s - loss: \n",
      "Epoch 4/30\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3265\n",
      "Epoch 5/30\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3276A:  - ETA: 0s - loss: 0.327\n",
      "Epoch 6/30\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3263A: 0s - loss: 0.326\n",
      "Epoch 7/30\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3259\n",
      "Epoch 8/30\n",
      "196/363 [===============>..............] - ETA: 0s - loss: 0.3173- ETA: 1s - l"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\MIGUEL~1\\AppData\\Local\\Temp/ipykernel_14080/577377571.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m                    \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                    \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m                    callbacks = [checkpoint_cb])\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Miguel Angel\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Miguel Angel\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1214\u001b[0m                 _r=1):\n\u001b[0;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1217\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Miguel Angel\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Miguel Angel\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    909\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 910\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    911\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Miguel Angel\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    947\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 949\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    950\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32mc:\\Users\\Miguel Angel\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   3130\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 3131\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   3132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3133\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Miguel Angel\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1958\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1959\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1960\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1962\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Miguel Angel\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    601\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 603\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    604\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mc:\\Users\\Miguel Angel\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 59\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     60\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"callback_model.h5\")\n",
    "history = model.fit(X_train,\n",
    "                   y_train,\n",
    "                   epochs=30,\n",
    "                   callbacks = [checkpoint_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stopping\n",
    "Interrumpe el entrenamiento cuando no ve progreso en el set de validación. Para ello tiene en cuenta un numero de epochs llamado `patience`. Se puede combinar con el callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3123 - val_loss: 0.3456\n",
      "Epoch 2/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3115 - val_loss: 0.3604\n",
      "Epoch 3/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3138 - val_loss: 0.3415\n",
      "Epoch 4/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3085 - val_loss: 0.3468\n",
      "Epoch 5/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3132 - val_loss: 0.3409\n",
      "Epoch 6/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3097 - val_loss: 0.4084\n",
      "Epoch 7/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3158 - val_loss: 0.3983\n",
      "Epoch 8/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3130 - val_loss: 0.3430\n",
      "Epoch 9/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3093 - val_loss: 0.3408\n",
      "Epoch 10/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3078 - val_loss: 0.3398\n",
      "Epoch 11/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3090 - val_loss: 0.3428\n",
      "Epoch 12/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3070 - val_loss: 0.3390\n",
      "Epoch 13/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3070 - val_loss: 0.3445\n",
      "Epoch 14/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3065 - val_loss: 0.3387\n",
      "Epoch 15/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3059 - val_loss: 0.3445\n",
      "Epoch 16/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3046 - val_loss: 0.3514\n",
      "Epoch 17/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3566 - val_loss: 0.3729\n",
      "Epoch 18/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3259 - val_loss: 0.3512\n",
      "Epoch 19/50\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3209 - val_loss: 0.3556\n"
     ]
    }
   ],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=5)\n",
    "history = model.fit(X_train,\n",
    "                   y_train,\n",
    "                   epochs=50,\n",
    "                    validation_data = (X_valid, y_valid),\n",
    "                   callbacks = [early_stopping_cb])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "90139cb9a825bf3d63f6f6704e828dbd1ff7edbd4d0c6e906a71235d6efc74af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
