{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis\n",
    "En este notebook vas a ver un ejemplo de los procesos necesarios para realizar un análisis de sentimientos sobre Tweets. Para ello tendremos que seguir los siguientes pasos:\n",
    "1. Conseguir un Corpus: no es más que una base de datos de texto etiquetado\n",
    "2. Limpiar los datos\n",
    "3. Entrenar un modelo con el corpus\n",
    "4. Atacar a la API de Twitter\n",
    "5. Predecir los nuevos Tweets\n",
    "\n",
    "**Estos programas son muy útiles en campañas de marketing, para monitorizar el lanzamiento de un nuevo producto, realizar seguimiento en Twitter de eventos, o simplemente tener monitorizadas ciertas cuentas o hashtags para tener un programa de análisis real time.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Corpus\n",
    "Para conseguir el corpus tendremos que registrarnos en la [página del TASS](http://tass.sepln.org/tass_data/download.php), que se trata de una asociación de análisis semántico que encargada de recopilar texto y mantenerlo etiquetado. \n",
    "\n",
    "Para datasets en ingles lo tenemos más fácil ya que con librerías como [TextBlob](https://textblob.readthedocs.io/en/dev/) podemos predecir directamente la polaridad del Tweet, con modelos ya preentrenados. En el caso del castellano necesitamos acudir a un corpus etiquetado para entrenar nuestro modelo.\n",
    "\n",
    "Registrate en el TASS y accede a sus corpus a través de un link que te llegará al correo tras el registro.\n",
    "\n",
    "![imagen](img/tass_register.png)\n",
    "\n",
    "\n",
    "Una vez estes registrado, descárgate el corpus de tweets en español de entrenamiento. En este punto lo ideal es coger un corpus que se adapte lo máximo posible a los tipos de tweets que intentamos predecir, es decir, si queremos predecir tweets sobre política, procurar elegir un corpus que tenga vocabulario de política.\n",
    "\n",
    "En este notebook se va a elegir un corpus genérico con no demasiados registros para aligerar la limpieza y entrenamiento de los modelos.\n",
    "\n",
    "![imagen](img/download_train_spanish.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importamos librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leemos el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = ET.parse('data/general-train-tagged.xml')\n",
    "root = tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7219, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Content</th>\n",
       "      <th>Date</th>\n",
       "      <th>Lang</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ccifuentes</td>\n",
       "      <td>Salgo de #VeoTV , que día más largoooooo...</td>\n",
       "      <td>2011-12-02T00:47:55</td>\n",
       "      <td>es</td>\n",
       "      <td>NONE</td>\n",
       "      <td>AGREEMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CarmendelRiego</td>\n",
       "      <td>@PauladeLasHeras No te libraras de ayudar me/n...</td>\n",
       "      <td>2011-12-02T00:49:40</td>\n",
       "      <td>es</td>\n",
       "      <td>NEU</td>\n",
       "      <td>DISAGREEMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CarmendelRiego</td>\n",
       "      <td>@marodriguezb Gracias MAR</td>\n",
       "      <td>2011-12-02T00:57:40</td>\n",
       "      <td>es</td>\n",
       "      <td>P</td>\n",
       "      <td>AGREEMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mgilguerrero</td>\n",
       "      <td>Off pensando en el regalito Sinde, la que se v...</td>\n",
       "      <td>2011-12-02T02:33:37</td>\n",
       "      <td>es</td>\n",
       "      <td>N+</td>\n",
       "      <td>AGREEMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>paurubio</td>\n",
       "      <td>Conozco a alguien q es adicto al drama! Ja ja ...</td>\n",
       "      <td>2011-12-02T02:59:03</td>\n",
       "      <td>es</td>\n",
       "      <td>P+</td>\n",
       "      <td>AGREEMENT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             User                                            Content  \\\n",
       "0      ccifuentes        Salgo de #VeoTV , que día más largoooooo...   \n",
       "1  CarmendelRiego  @PauladeLasHeras No te libraras de ayudar me/n...   \n",
       "2  CarmendelRiego                          @marodriguezb Gracias MAR   \n",
       "3    mgilguerrero  Off pensando en el regalito Sinde, la que se v...   \n",
       "4        paurubio  Conozco a alguien q es adicto al drama! Ja ja ...   \n",
       "\n",
       "                  Date Lang Polarity          Type  \n",
       "0  2011-12-02T00:47:55   es     NONE     AGREEMENT  \n",
       "1  2011-12-02T00:49:40   es      NEU  DISAGREEMENT  \n",
       "2  2011-12-02T00:57:40   es        P     AGREEMENT  \n",
       "3  2011-12-02T02:33:37   es       N+     AGREEMENT  \n",
       "4  2011-12-02T02:59:03   es       P+     AGREEMENT  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dict = {\n",
    "    'User': [],\n",
    "    'Content': [],\n",
    "    'Date': [],\n",
    "    'Lang': [],\n",
    "    'Polarity': [],\n",
    "    'Type': []\n",
    "}\n",
    "\n",
    "for i in root.iter('tweet'):\n",
    "    user = i.find('user').text\n",
    "    content = i.find('content').text\n",
    "    date = i.find('date').text\n",
    "    lang = i.find('lang').text\n",
    "    polarity = i.find('sentiments').find('polarity').find('value').text\n",
    "    tweet_type = i.find('sentiments').find('polarity').find('type').text\n",
    "    \n",
    "    raw_dict['User'].append(user)\n",
    "    raw_dict['Content'].append(content)\n",
    "    raw_dict['Date'].append(date)\n",
    "    raw_dict['Lang'].append(lang)\n",
    "    raw_dict['Polarity'].append(polarity)\n",
    "    raw_dict['Type'].append(tweet_type)\n",
    "    \n",
    "df = pd.DataFrame(raw_dict)\n",
    "print(df.shape)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Content</th>\n",
       "      <th>Date</th>\n",
       "      <th>Lang</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ccifuentes</td>\n",
       "      <td>Salgo de #VeoTV , que día más largoooooo...</td>\n",
       "      <td>2011-12-02T00:47:55</td>\n",
       "      <td>es</td>\n",
       "      <td>NONE</td>\n",
       "      <td>AGREEMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CarmendelRiego</td>\n",
       "      <td>@PauladeLasHeras No te libraras de ayudar me/nos. Besos y gracias</td>\n",
       "      <td>2011-12-02T00:49:40</td>\n",
       "      <td>es</td>\n",
       "      <td>NEU</td>\n",
       "      <td>DISAGREEMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CarmendelRiego</td>\n",
       "      <td>@marodriguezb Gracias MAR</td>\n",
       "      <td>2011-12-02T00:57:40</td>\n",
       "      <td>es</td>\n",
       "      <td>P</td>\n",
       "      <td>AGREEMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mgilguerrero</td>\n",
       "      <td>Off pensando en el regalito Sinde, la que se va de la SGAE cuando se van sus corruptos. Intento no sacar conclusiones (lo intento)</td>\n",
       "      <td>2011-12-02T02:33:37</td>\n",
       "      <td>es</td>\n",
       "      <td>N+</td>\n",
       "      <td>AGREEMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>paurubio</td>\n",
       "      <td>Conozco a alguien q es adicto al drama! Ja ja ja te suena d algo!</td>\n",
       "      <td>2011-12-02T02:59:03</td>\n",
       "      <td>es</td>\n",
       "      <td>P+</td>\n",
       "      <td>AGREEMENT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             User  \\\n",
       "0      ccifuentes   \n",
       "1  CarmendelRiego   \n",
       "2  CarmendelRiego   \n",
       "3    mgilguerrero   \n",
       "4        paurubio   \n",
       "\n",
       "                                                                                                                              Content  \\\n",
       "0                                                                                         Salgo de #VeoTV , que día más largoooooo...   \n",
       "1                                                                   @PauladeLasHeras No te libraras de ayudar me/nos. Besos y gracias   \n",
       "2                                                                                                           @marodriguezb Gracias MAR   \n",
       "3  Off pensando en el regalito Sinde, la que se va de la SGAE cuando se van sus corruptos. Intento no sacar conclusiones (lo intento)   \n",
       "4                                                                   Conozco a alguien q es adicto al drama! Ja ja ja te suena d algo!   \n",
       "\n",
       "                  Date Lang Polarity          Type  \n",
       "0  2011-12-02T00:47:55   es     NONE     AGREEMENT  \n",
       "1  2011-12-02T00:49:40   es      NEU  DISAGREEMENT  \n",
       "2  2011-12-02T00:57:40   es        P     AGREEMENT  \n",
       "3  2011-12-02T02:33:37   es       N+     AGREEMENT  \n",
       "4  2011-12-02T02:59:03   es       P+     AGREEMENT  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columna de polaridad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NONE', 'NEU', 'P', 'N+', 'P+', 'N'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vemos los valores unicos de la columna de polaridad\n",
    "df.Polarity.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x = 'Polarity', data=df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Columna de tipo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x = 'Type', data=df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Limpieza de datos\n",
    "#### Polaridad\n",
    "Vamos a clasificar los Tweets como buenos o malos, por lo que haremos la siguiente agrupación de la polaridad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polaridad_fun(x):\n",
    "    if x in ('P', 'P+'):\n",
    "        return 0\n",
    "    elif x in ('N', 'N+'):\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['P', 'N+', 'P+', 'N'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nos cargamos los NONE y los neutros\n",
    "df = df[~df['Polarity'].isin(['NONE', 'NEU'])]\n",
    "df['Polarity'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pasamos la columna a 1s y 0s. Y el tipo\n",
    "df['Polarity'] = df['Polarity'].apply(polaridad_fun)\n",
    "df['Polarity'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Idioma\n",
    "Nos quedamos con los tweets en español. Si no tuviésemos esa columna podríamos acudir a librerías como `langid` o `langdetect`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtramos los tweets en español\n",
    "df = df[df['Lang'] == 'es']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5066, 6)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vemos con cuantos registros nos hemos quedado despues del filtrado\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5052, 6)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Eliminamos los duplicados\n",
    "df.drop_duplicates(subset = 'Content', inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Signos de puntuación\n",
    "Eliminamos signos de puntuación: puntos, comas, interrogaciones, paréntesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2                                                                                                                    @marodriguezb Gracias MAR\n",
       "3           Off pensando en el regalito Sinde, la que se va de la SGAE cuando se van sus corruptos. Intento no sacar conclusiones (lo intento)\n",
       "4                                                                            Conozco a alguien q es adicto al drama! Ja ja ja te suena d algo!\n",
       "6                                                                        Toca @crackoviadeTV3 . Grabación dl especial Navideño...Mari crismas!\n",
       "8    Buen día todos! Lo primero mandar un abrazo grande a Miguel y a su familia @libertadmontes Hoy podría ser un día para la grandeza humana.\n",
       "Name: Content, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Content'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2                                                                                                                  marodriguezb gracias mar\n",
       "3            off pensando en el regalito sinde la que se va de la sgae cuando se van sus corruptos intento no sacar conclusiones lo intento\n",
       "4                                                                           conozco a alguien q es adicto al drama ja ja ja te suena d algo\n",
       "6                                                                            toca crackoviadetv  grabación dl especial navideñomari crismas\n",
       "8    buen día todos lo primero mandar un abrazo grande a miguel y a su familia libertadmontes hoy podría ser un día para la grandeza humana\n",
       "Name: Content, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "signos = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\?)|(\\¿)|(\\@)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(\\d+)\")\n",
    "\n",
    "def signs_tweets(tweet):\n",
    "    return signos.sub('', tweet.lower())\n",
    "\n",
    "df['Content'] = df['Content'].apply(signs_tweets)\n",
    "df['Content'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eliminamos links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_links(df):\n",
    "    return \" \".join(['{link}' if ('http') in word else word for word in df.split()])\n",
    "\n",
    "df['Content'] = df['Content'].apply(remove_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Otros\n",
    "Podríamos hacer un preprocesado mucho más fino:\n",
    "1. Hashtags\n",
    "2. Menciones\n",
    "3. Abreviaturas\n",
    "4. Faltas de ortografía\n",
    "5. Risas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modelo\n",
    "Para montar el modelo tendremos que seguir los siguientes pasos\n",
    "1. Eliminamos las stopwords\n",
    "2. Aplicamos un stemmer, SnowBall por ejemplo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "spanish_stopwords = stopwords.words('spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(df):\n",
    "    return \" \".join([word for word in df.split() if word not in spanish_stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Content</th>\n",
       "      <th>Date</th>\n",
       "      <th>Lang</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CarmendelRiego</td>\n",
       "      <td>marodriguezb gracias mar</td>\n",
       "      <td>2011-12-02T00:57:40</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "      <td>AGREEMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mgilguerrero</td>\n",
       "      <td>off pensando regalito sinde va sgae van corruptos intento sacar conclusiones intento</td>\n",
       "      <td>2011-12-02T02:33:37</td>\n",
       "      <td>es</td>\n",
       "      <td>1</td>\n",
       "      <td>AGREEMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>paurubio</td>\n",
       "      <td>conozco alguien q adicto drama ja ja ja suena d</td>\n",
       "      <td>2011-12-02T02:59:03</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "      <td>AGREEMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Carlos_Latre</td>\n",
       "      <td>toca crackoviadetv grabación dl especial navideñomari crismas</td>\n",
       "      <td>2011-12-02T07:00:50</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "      <td>AGREEMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>nacho_uriarte</td>\n",
       "      <td>buen día primero mandar abrazo grande miguel familia libertadmontes hoy podría ser día grandeza humana</td>\n",
       "      <td>2011-12-02T07:45:05</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "      <td>AGREEMENT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             User  \\\n",
       "2  CarmendelRiego   \n",
       "3    mgilguerrero   \n",
       "4        paurubio   \n",
       "6    Carlos_Latre   \n",
       "8   nacho_uriarte   \n",
       "\n",
       "                                                                                                  Content  \\\n",
       "2                                                                                marodriguezb gracias mar   \n",
       "3                    off pensando regalito sinde va sgae van corruptos intento sacar conclusiones intento   \n",
       "4                                                         conozco alguien q adicto drama ja ja ja suena d   \n",
       "6                                           toca crackoviadetv grabación dl especial navideñomari crismas   \n",
       "8  buen día primero mandar abrazo grande miguel familia libertadmontes hoy podría ser día grandeza humana   \n",
       "\n",
       "                  Date Lang  Polarity       Type  \n",
       "2  2011-12-02T00:57:40   es         0  AGREEMENT  \n",
       "3  2011-12-02T02:33:37   es         1  AGREEMENT  \n",
       "4  2011-12-02T02:59:03   es         0  AGREEMENT  \n",
       "6  2011-12-02T07:00:50   es         0  AGREEMENT  \n",
       "8  2011-12-02T07:45:05   es         0  AGREEMENT  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Content'] = df['Content'].apply(remove_stopwords)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2                                                                        marodriguezb graci mar\n",
       "3                         off pens regalit sind va sga van corrupt intent sac conclusion intent\n",
       "4                                                     conozc algui q adict dram ja ja ja suen d\n",
       "6                                    toc crackoviadetv grabacion dl especial navideñomari crism\n",
       "8    buen dia primer mand abraz grand miguel famili libertadmont hoy podr ser dia grandez human\n",
       "Name: Content, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "def spanish_stemmer(x):\n",
    "    stemmer = SnowballStemmer('spanish')\n",
    "    return \" \".join([stemmer.stem(word) for word in x.split()])\n",
    "\n",
    "df['Content'] = df['Content'].apply(spanish_stemmer)\n",
    "df['Content'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seleccionamos columnas\n",
    "Nos quedamos con las columnas que nos interesan para el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['Content', 'Polarity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/output/data_processed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorizamos el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Montamos Pipeline\n",
    "Modelos que suelen funcionar bien con pocas observaciones y muchas features son la Regresión logística el LinearSVC o Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', vectorizer),\n",
    "    ('cls', LogisticRegression())\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 1 , 1.9),\n",
    "    'vect__min_df': (5, 10, 20,50),\n",
    "    'vect__max_features': (500, 1000),\n",
    "    \"cls__penalty\": [\"l1\",\"l2\"], # Regularizaciones L1 y L2.\n",
    "    \"cls__C\": [0.1, 0.5, 1.0, 5.0] # Cuanta regularizacion queremos\n",
    "\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline,\n",
    "                          parameters,\n",
    "                          cv = 5,\n",
    "                          n_jobs = -1,\n",
    "                          scoring = 'accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Miguel Angel\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "800 fits failed out of a total of 960.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "160 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Miguel Angel\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Miguel Angel\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Miguel Angel\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Miguel Angel\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 449, in _check_solver\n",
      "    % (solver, penalty)\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "320 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Miguel Angel\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Miguel Angel\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\pipeline.py\", line 390, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"c:\\Users\\Miguel Angel\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\pipeline.py\", line 355, in _fit\n",
      "    **fit_params_steps[name],\n",
      "  File \"c:\\Users\\Miguel Angel\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"c:\\Users\\Miguel Angel\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\pipeline.py\", line 893, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"c:\\Users\\Miguel Angel\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1344, in fit_transform\n",
      "    raise ValueError(\"max_df corresponds to < documents than min_df\")\n",
      "ValueError: max_df corresponds to < documents than min_df\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "320 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Miguel Angel\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Miguel Angel\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\pipeline.py\", line 390, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"c:\\Users\\Miguel Angel\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\pipeline.py\", line 355, in _fit\n",
      "    **fit_params_steps[name],\n",
      "  File \"c:\\Users\\Miguel Angel\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"c:\\Users\\Miguel Angel\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\pipeline.py\", line 893, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"c:\\Users\\Miguel Angel\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1313, in fit_transform\n",
      "    self._validate_params()\n",
      "  File \"c:\\Users\\Miguel Angel\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1264, in _validate_params\n",
      "    check_scalar(self.max_df, \"max_df\", numbers.Real, min_val=0.0, max_val=1.0)\n",
      "  File \"c:\\Users\\Miguel Angel\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 1330, in check_scalar\n",
      "    f\"{name} == {x}, must be\"\n",
      "ValueError: max_df == 1.9, must be <= 1.0.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Miguel Angel\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.73179324 0.73218889 0.72090294 0.67873843 0.74109626 0.7365418\n",
      " 0.72090294 0.67873843        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.73713645 0.73535525 0.72347622 0.67893723 0.75138741 0.74307332\n",
      " 0.72347622 0.67893723        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.73931349 0.73832202 0.72367306 0.68051983 0.74841692 0.74544662\n",
      " 0.72367306 0.68051983        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.73435928 0.73099372 0.72485961 0.68012437 0.74030202 0.73832222\n",
      " 0.72485961 0.68012437        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  category=UserWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('vect',\n",
       "                                        CountVectorizer(ngram_range=(1, 2))),\n",
       "                                       ('cls', LogisticRegression())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'cls__C': [0.1, 0.5, 1.0, 5.0],\n",
       "                         'cls__penalty': ['l1', 'l2'],\n",
       "                         'vect__max_df': (0.5, 1, 1.9),\n",
       "                         'vect__max_features': (500, 1000),\n",
       "                         'vect__min_df': (5, 10, 20, 50)},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(df['Content'], df['Polarity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'cls__C': 0.5, 'cls__penalty': 'l2', 'vect__max_df': 0.5, 'vect__max_features': 1000, 'vect__min_df': 5}\n",
      "Best acc: 0.7513874117382064\n",
      "Best model: Pipeline(steps=[('vect',\n",
      "                 CountVectorizer(max_df=0.5, max_features=1000, min_df=5,\n",
      "                                 ngram_range=(1, 2))),\n",
      "                ('cls', LogisticRegression(C=0.5))])\n"
     ]
    }
   ],
   "source": [
    "print(\"Best params:\", grid_search.best_params_)\n",
    "print(\"Best acc:\", grid_search.best_score_)\n",
    "print(\"Best model:\", grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.5)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_estimator_['cls']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Guardamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('data/output/finished_model.model', \"wb\") as archivo_salida:\n",
    "    pickle.dump(grid_search.best_estimator_, archivo_salida)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Predicciones\n",
    "Realizar una predicción con un tweet que escojas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/output/finished_model.model', \"rb\") as archivo_entrada:\n",
    "    pipeline_importada = pickle.load(archivo_entrada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect',\n",
       "                 CountVectorizer(max_df=0.5, max_features=1000, min_df=5,\n",
       "                                 ngram_range=(1, 2))),\n",
       "                ('cls', LogisticRegression(C=0.5))])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_importada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leemos el pipeline con el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pd.Series('Decepcionando al personal. A quienes parece mal que a Javier Marías y a mí nos gustara mucho John Wayne (en sus películas) podría serles útil esto, escrito hace un par de años.')\n",
    "test_clean = pd.DataFrame(text, columns=['content'])\n",
    "\n",
    "# Signos de puntuacion\n",
    "test_clean['content_clean'] = test_clean['content'].apply(signs_tweets)\n",
    "\n",
    "# Eliminamos links\n",
    "test_clean['content_clean'] = test_clean['content_clean'].apply(remove_links)\n",
    "\n",
    "# Nos cargamos stopwords\n",
    "test_clean['content_clean'] = test_clean['content_clean'].apply(remove_stopwords)\n",
    "\n",
    "# Aplicamos el Stemmer\n",
    "test_clean['content_clean'] = test_clean['content_clean'].apply(spanish_stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>content_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decepcionando al personal. A quienes parece mal que a Javier Marías y a mí nos gustara mucho John Wayne (en sus películas) podría serles útil esto, escrito hace un par de años.</td>\n",
       "      <td>decepcion personal parec mal javi mar gust john wayn pelicul podr serl util escrit hac par años</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                            content  \\\n",
       "0  Decepcionando al personal. A quienes parece mal que a Javier Marías y a mí nos gustara mucho John Wayne (en sus películas) podría serles útil esto, escrito hace un par de años.   \n",
       "\n",
       "                                                                                     content_clean  \n",
       "0  decepcion personal parec mal javi mar gust john wayn pelicul podr serl util escrit hac par años  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicciones de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    bombarde #gernik part aviacion nazi fascist episodi cruent guerr civil años despues record dia tod victim memori imprescind segu constru socied democrat paz\n",
       "Name: content_clean, dtype: object"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_clean['content_clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>content_clean</th>\n",
       "      <th>Polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decepcionando al personal. A quienes parece mal que a Javier Marías y a mí nos gustara mucho John Wayne (en sus películas) podría serles útil esto, escrito hace un par de años.</td>\n",
       "      <td>decepcion personal parec mal javi mar gust john wayn pelicul podr serl util escrit hac par años</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                            content  \\\n",
       "0  Decepcionando al personal. A quienes parece mal que a Javier Marías y a mí nos gustara mucho John Wayne (en sus películas) podría serles útil esto, escrito hace un par de años.   \n",
       "\n",
       "                                                                                     content_clean  \\\n",
       "0  decepcion personal parec mal javi mar gust john wayn pelicul podr serl util escrit hac par años   \n",
       "\n",
       "   Polarity  \n",
       "0         1  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = pipeline_importada.predict(test_clean['content_clean'])\n",
    "test_clean['Polarity'] = pd.Series(predictions)\n",
    "test_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>content_clean</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Polarity_Pos</th>\n",
       "      <th>Polarity_Neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decepcionando al personal. A quienes parece mal que a Javier Marías y a mí nos gustara mucho John Wayne (en sus películas) podría serles útil esto, escrito hace un par de años.</td>\n",
       "      <td>decepcion personal parec mal javi mar gust john wayn pelicul podr serl util escrit hac par años</td>\n",
       "      <td>1</td>\n",
       "      <td>0.07601</td>\n",
       "      <td>0.92399</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                            content  \\\n",
       "0  Decepcionando al personal. A quienes parece mal que a Javier Marías y a mí nos gustara mucho John Wayne (en sus películas) podría serles útil esto, escrito hace un par de años.   \n",
       "\n",
       "                                                                                     content_clean  \\\n",
       "0  decepcion personal parec mal javi mar gust john wayn pelicul podr serl util escrit hac par años   \n",
       "\n",
       "   Polarity  Polarity_Pos  Polarity_Neg  \n",
       "0         1       0.07601       0.92399  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = pipeline_importada.predict_proba(test_clean['content_clean'])\n",
    "test_clean['Polarity_Pos'] = pd.Series(predictions[0][0])\n",
    "test_clean['Polarity_Neg'] = pd.Series(predictions[0][1])\n",
    "test_clean"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "90139cb9a825bf3d63f6f6704e828dbd1ff7edbd4d0c6e906a71235d6efc74af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
